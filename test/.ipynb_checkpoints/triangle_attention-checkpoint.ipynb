{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8qYrjej8CNR"
   },
   "source": [
    "Some comments from [@sokrypton](https://github.com/sokrypton/)\n",
    "\n",
    "*   The 3rd track in Rosettafold is essentially doing what alphafold triangle\n",
    "attention is doing. Adding a 3D bias to the attention. In Rosettafold this is done explicitly via evolving 3D coordinates.\n",
    "\n",
    "*   In alphafold this is done implicitly by essentially learning triangle inequality. Often if your distance matrix satisfies the condition that all three distances form a triangle, it is possible to embed structure in 3D space.\n",
    "\n",
    "*   The potential benefit of 3D track is that you remove the expensive L^3 operation and replace this with L^2.\n",
    "\n",
    "*   The potential negative part is that you can get stuck in local minimum as explicit 3D structure maybe harder to refine iteratively compared to a 2D distance matrix, which is not explicitly forced to be a valid distance matrix at each step of the evoformer.\n",
    "\n",
    "*   So it possible alphafold learned to not enforce triangle inequality at early layers, but then enforces them in latter layers.\n",
    "\n",
    "See also [these AI conversation logs](https://poe.com/s/03iamy0dmmJF3023ccNp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDYHKSr8aNq7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from typing import Dict, Optional, Callable , List, Any\n",
    "from functools import partial\n",
    "from collections.abc import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttcdqLj7sIIi"
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/dptech-corp/Uni-Core/blob/854b8890daa5722ba4e30eed1973564671611f6f/unicore/utils.py\n",
    "\n",
    "def dict_map(fn, dic, leaf_type):\n",
    "    new_dict = {}\n",
    "    for k, v in dic.items():\n",
    "        if type(v) is dict:\n",
    "            new_dict[k] = dict_map(fn, v, leaf_type)\n",
    "        else:\n",
    "            new_dict[k] = tree_map(fn, v, leaf_type)\n",
    "    return new_dict\n",
    "\n",
    "def tree_map(fn, tree, leaf_type):\n",
    "    if isinstance(tree, dict):\n",
    "        return dict_map(fn, tree, leaf_type)\n",
    "    elif isinstance(tree, list):\n",
    "        return [tree_map(fn, x, leaf_type) for x in tree]\n",
    "    elif isinstance(tree, tuple):\n",
    "        return tuple([tree_map(fn, x, leaf_type) for x in tree])\n",
    "    elif isinstance(tree, leaf_type):\n",
    "        try:\n",
    "            return fn(tree)\n",
    "        except:\n",
    "            raise ValueError(f\"cannot apply {fn} on {tree}.\")\n",
    "    else:\n",
    "        raise ValueError(f\"{type(tree)} not supported\")\n",
    "tensor_tree_map = partial(tree_map, leaf_type=torch.Tensor)\n",
    "\n",
    "def permute_final_dims(tensor: torch.Tensor, inds: List[int]):\n",
    "    zero_index = -1 * len(inds)\n",
    "    first_inds = list(range(len(tensor.shape[:zero_index])))\n",
    "    return tensor.permute(first_inds + [zero_index + i for i in inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-TVguLBeKbF"
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/dptech-corp/Uni-Fold/blob/1a301710392ecf97991aebc3276aad9d0f77178f/unifold/modules/common.py#L299\n",
    "\n",
    "def chunk_layer(\n",
    "    layer: Callable,\n",
    "    inputs: Dict[str, Any],\n",
    "    chunk_size: int,\n",
    "    num_batch_dims: int,\n",
    ") -> Any:\n",
    "    # TODO: support inplace add to output\n",
    "    if not (len(inputs) > 0):\n",
    "        raise ValueError(\"Must provide at least one input\")\n",
    "    def _dict_get_shapes(input):\n",
    "        shapes = []\n",
    "        if type(input) is torch.Tensor:\n",
    "            shapes.append(input.shape)\n",
    "        elif type(input) is dict:\n",
    "            for v in input.values():\n",
    "                shapes.extend(_dict_get_shapes(v))\n",
    "        elif isinstance(input, Iterable):\n",
    "            for v in input:\n",
    "                shapes.extend(_dict_get_shapes(v))\n",
    "        else:\n",
    "            raise ValueError(\"Not supported\")\n",
    "        return shapes\n",
    "    inputs = {k: v for k, v in inputs.items() if v is not None}\n",
    "    initial_dims = [shape[:num_batch_dims] for shape in _dict_get_shapes(inputs)]\n",
    "    orig_batch_dims = tuple([max(s) for s in zip(*initial_dims)])\n",
    "    flat_batch_dim = 1\n",
    "    for d in orig_batch_dims:\n",
    "        flat_batch_dim *= d\n",
    "    num_chunks = (flat_batch_dim + chunk_size - 1) // chunk_size\n",
    "    def _flat_inputs(t):\n",
    "        t = t.view(-1, *t.shape[num_batch_dims:])\n",
    "        assert (\n",
    "            t.shape[0] == flat_batch_dim or t.shape[0] == 1\n",
    "        ), \"batch dimension must be 1 or equal to the flat batch dimension\"\n",
    "        return t\n",
    "    flat_inputs = tensor_tree_map(_flat_inputs, inputs)\n",
    "    out = None\n",
    "    for i in range(num_chunks):\n",
    "        chunk_start = i * chunk_size\n",
    "        chunk_end = min((i + 1) * chunk_size, flat_batch_dim)\n",
    "        def select_chunk(t):\n",
    "            if t.shape[0] == 1:\n",
    "                return t[0:1]\n",
    "            else:\n",
    "                return t[chunk_start:chunk_end]\n",
    "        chunkes = tensor_tree_map(select_chunk, flat_inputs)\n",
    "        output_chunk = layer(**chunkes)\n",
    "        if out is None:\n",
    "            out = tensor_tree_map(\n",
    "                lambda t: t.new_zeros((flat_batch_dim,) + t.shape[1:]), output_chunk\n",
    "            )\n",
    "        out_type = type(output_chunk)\n",
    "        if out_type is tuple:\n",
    "            for x, y in zip(out, output_chunk):\n",
    "                x[chunk_start:chunk_end] = y\n",
    "        elif out_type is torch.Tensor:\n",
    "            out[chunk_start:chunk_end] = output_chunk\n",
    "        else:\n",
    "            raise ValueError(\"Not supported\")\n",
    "    reshape = lambda t: t.view(orig_batch_dims + t.shape[1:])\n",
    "    out = tensor_tree_map(reshape, out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcjUS_1hz41-"
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/dptech-corp/Uni-Fold/blob/1a301710392ecf97991aebc3276aad9d0f77178f/unifold/modules/common.py\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_out: int,\n",
    "        bias: bool = True,\n",
    "        init: str = \"default\",\n",
    "    ):\n",
    "        super(Linear, self).__init__(d_in, d_out, bias=bias)\n",
    "\n",
    "        self.use_bias = bias\n",
    "\n",
    "        if self.use_bias:\n",
    "            with torch.no_grad():\n",
    "                self.bias.fill_(0)\n",
    "\n",
    "        if init == \"default\":\n",
    "            self._trunc_normal_init(1.0)\n",
    "        elif init == \"relu\":\n",
    "            self._trunc_normal_init(2.0)\n",
    "        elif init == \"glorot\":\n",
    "            self._glorot_uniform_init()\n",
    "        elif init == \"gating\":\n",
    "            self._zero_init(self.use_bias)\n",
    "        elif init == \"normal\":\n",
    "            self._normal_init()\n",
    "        elif init == \"final\":\n",
    "            self._zero_init(False)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid init method.\")\n",
    "\n",
    "    def _trunc_normal_init(self, scale=1.0):\n",
    "        # Constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n",
    "        TRUNCATED_NORMAL_STDDEV_FACTOR = 0.87962566103423978\n",
    "        _, fan_in = self.weight.shape\n",
    "        scale = scale / max(1, fan_in)\n",
    "        std = (scale**0.5) / TRUNCATED_NORMAL_STDDEV_FACTOR\n",
    "        nn.init.trunc_normal_(self.weight, mean=0.0, std=std)\n",
    "\n",
    "    def _glorot_uniform_init(self):\n",
    "        nn.init.xavier_uniform_(self.weight, gain=1)\n",
    "\n",
    "    def _zero_init(self, use_bias=True):\n",
    "        with torch.no_grad():\n",
    "            self.weight.fill_(0.0)\n",
    "            if use_bias:\n",
    "                with torch.no_grad():\n",
    "                    self.bias.fill_(1.0)\n",
    "\n",
    "    def _normal_init(self):\n",
    "        torch.nn.init.kaiming_normal_(self.weight, nonlinearity=\"linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03TPGvRz1NwH"
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/dptech-corp/Uni-Core/blob/44f6386f4dcd7137fc1e5d5e768117d635d64a26/unicore/modules/layer_norm.py\n",
    "\n",
    "# Copyright (c) DP Technology.\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "import torch\n",
    "import numbers\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\"\"\"\n",
    "try:\n",
    "    import unicore_fused_layernorm\n",
    "    import unicore_fused_layernorm_backward_gamma_beta\n",
    "    HAS_LAYER_NORM = True\n",
    "except:\n",
    "    print(\"fused_layer_norm is not installed corrected\")\n",
    "    HAS_LAYER_NORM = False\n",
    "\n",
    "if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 7:\n",
    "    HAS_LAYER_NORM = False\n",
    "\"\"\"\n",
    "\n",
    "HAS_LAYER_NORM = False\n",
    "\n",
    "class FusedLayerNormFastFunction(torch.autograd.Function):\n",
    "  @staticmethod\n",
    "  def forward(ctx, input, weight, bias, normalized_shape, eps):\n",
    "    ctx.normalized_shape = normalized_shape\n",
    "    ctx.eps = eps\n",
    "    input = input.contiguous()\n",
    "    weight = weight.contiguous()\n",
    "    bias = bias.contiguous()\n",
    "    output, mean, invvar = unicore_fused_layernorm.forward(\n",
    "        input, ctx.normalized_shape, weight, bias, ctx.eps)\n",
    "    ctx.save_for_backward(input, weight, bias, mean, invvar)\n",
    "    return output\n",
    "  @staticmethod\n",
    "  def backward(ctx, grad_output):\n",
    "    input_, weight_, bias_, mean, invvar = ctx.saved_tensors\n",
    "    grad_input = grad_weight = grad_bias = None\n",
    "    grad_input = unicore_fused_layernorm.backward(\n",
    "        grad_output.contiguous(), mean, invvar,\n",
    "        input_, ctx.normalized_shape,\n",
    "        weight_, bias_, ctx.eps)\n",
    "    grad_weight, grad_bias = unicore_fused_layernorm_backward_gamma_beta.backward(\n",
    "        grad_output.contiguous(), mean, invvar,\n",
    "        input_, ctx.normalized_shape,\n",
    "        weight_, bias_, ctx.eps)\n",
    "    return grad_input, grad_weight, grad_bias, None, None\n",
    "\n",
    "FUSED_LAYER_NORM_SUPPORT_DIM = set([64, 128, 192, 256, 320, 384, 512, 640, 768, 1024, 1280, 1536, 1792, 2048, 2560, 5120])\n",
    "\n",
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        if isinstance(normalized_shape, numbers.Integral):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = torch.Size(normalized_shape)\n",
    "        self.eps = eps\n",
    "        assert elementwise_affine\n",
    "        self.weight = Parameter(torch.Tensor(*normalized_shape))\n",
    "        self.bias = Parameter(torch.Tensor(*normalized_shape))\n",
    "        self.reset_parameters()\n",
    "        def torch_layer_norm(input):\n",
    "            return F.layer_norm(\n",
    "                input, self.normalized_shape, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)\n",
    "        def fused_layer_norm(input):\n",
    "            if input.is_cuda:\n",
    "                return FusedLayerNormFastFunction.apply(\n",
    "                    input, self.weight.type(input.dtype), self.bias.type(input.dtype), self.normalized_shape, self.eps)\n",
    "            else:\n",
    "                return F.layer_norm(\n",
    "                    input, self.normalized_shape, self.weight.type(input.dtype), self.bias.type(input.dtype), self.eps)\n",
    "        self.func = torch_layer_norm if (not HAS_LAYER_NORM or normalized_shape[0] not in FUSED_LAYER_NORM_SUPPORT_DIM) else fused_layer_norm\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        init.ones_(self.weight)\n",
    "        init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.func(input)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return '{normalized_shape}, eps={eps}, ' \\\n",
    "            'elementwise_affine=True'.format(**self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tydYQlP1wAG6"
   },
   "outputs": [],
   "source": [
    "# Reference : https://github.com/dptech-corp/Uni-Fold/blob/1a301710392ecf97991aebc3276aad9d0f77178f/unifold/modules/triangle_multiplication.py\n",
    "\n",
    "from functools import partialmethod\n",
    "\n",
    "class TriangleMultiplication(nn.Module):\n",
    "    def __init__(self, d_pair, d_hid, outgoing=True):\n",
    "        super(TriangleMultiplication, self).__init__()\n",
    "        self.outgoing = outgoing\n",
    "\n",
    "        self.linear_ab_p = Linear(d_pair, d_hid * 2)\n",
    "        self.linear_ab_g = Linear(d_pair, d_hid * 2, init=\"gating\")\n",
    "\n",
    "        self.linear_g = Linear(d_pair, d_pair, init=\"gating\")\n",
    "        self.linear_z = Linear(d_hid, d_pair, init=\"final\")\n",
    "\n",
    "        self.layer_norm_in = LayerNorm(d_pair)\n",
    "        self.layer_norm_out = LayerNorm(d_hid)\n",
    "\n",
    "        self._alphafold_original_mode = False\n",
    "\n",
    "    def _chunk_2d(\n",
    "        self,\n",
    "        z: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        block_size: int = None,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        # avoid too small chunk size\n",
    "        # block_size = max(block_size, 256)\n",
    "        new_z = z.new_zeros(z.shape)\n",
    "        dim1 = z.shape[-3]\n",
    "\n",
    "        def _slice_linear(z, linear: Linear, a=True):\n",
    "            d_hid = linear.bias.shape[0] // 2\n",
    "            index = 0 if a else d_hid\n",
    "            p = (\n",
    "                nn.functional.linear(z, linear.weight[index : index + d_hid])\n",
    "                + linear.bias[index : index + d_hid]\n",
    "            )\n",
    "            return p\n",
    "\n",
    "        def _chunk_projection(z, mask, a=True):\n",
    "            p = _slice_linear(z, self.linear_ab_p, a) * mask\n",
    "            p *= torch.sigmoid(_slice_linear(z, self.linear_ab_g, a))\n",
    "            return p\n",
    "\n",
    "        num_chunk = (dim1 + block_size - 1) // block_size\n",
    "        for i in range(num_chunk):\n",
    "            chunk_start = i * block_size\n",
    "            chunk_end = min(chunk_start + block_size, dim1)\n",
    "            if self.outgoing:\n",
    "                a_chunk = _chunk_projection(\n",
    "                    z[..., chunk_start:chunk_end, :, :],\n",
    "                    mask[..., chunk_start:chunk_end, :, :],\n",
    "                    a=True,\n",
    "                )\n",
    "                a_chunk = permute_final_dims(a_chunk, (2, 0, 1))\n",
    "            else:\n",
    "                a_chunk = _chunk_projection(\n",
    "                    z[..., :, chunk_start:chunk_end, :],\n",
    "                    mask[..., :, chunk_start:chunk_end, :],\n",
    "                    a=True,\n",
    "                )\n",
    "                a_chunk = a_chunk.transpose(-1, -3)\n",
    "\n",
    "            for j in range(num_chunk):\n",
    "                j_chunk_start = j * block_size\n",
    "                j_chunk_end = min(j_chunk_start + block_size, dim1)\n",
    "                if self.outgoing:\n",
    "                    b_chunk = _chunk_projection(\n",
    "                        z[..., j_chunk_start:j_chunk_end, :, :],\n",
    "                        mask[..., j_chunk_start:j_chunk_end, :, :],\n",
    "                        a=False,\n",
    "                    )\n",
    "                    b_chunk = b_chunk.transpose(-1, -3)\n",
    "                else:\n",
    "                    b_chunk = _chunk_projection(\n",
    "                        z[..., :, j_chunk_start:j_chunk_end, :],\n",
    "                        mask[..., :, j_chunk_start:j_chunk_end, :],\n",
    "                        a=False,\n",
    "                    )\n",
    "                    b_chunk = permute_final_dims(b_chunk, (2, 0, 1))\n",
    "                x_chunk = torch.matmul(a_chunk, b_chunk)\n",
    "                del b_chunk\n",
    "                x_chunk = permute_final_dims(x_chunk, (1, 2, 0))\n",
    "                x_chunk = self.layer_norm_out(x_chunk)\n",
    "                x_chunk = self.linear_z(x_chunk)\n",
    "                x_chunk *= torch.sigmoid(\n",
    "                    self.linear_g(\n",
    "                        z[..., chunk_start:chunk_end, j_chunk_start:j_chunk_end, :]\n",
    "                    )\n",
    "                )\n",
    "                new_z[\n",
    "                    ..., chunk_start:chunk_end, j_chunk_start:j_chunk_end, :\n",
    "                ] = x_chunk\n",
    "                del x_chunk\n",
    "            del a_chunk\n",
    "        return new_z\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        z: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        block_size=None,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        if not self._alphafold_original_mode:\n",
    "            # divided by 1/sqrt(dim) for numerical stability\n",
    "            mask = mask * (mask.shape[-2] ** -0.5)\n",
    "\n",
    "        z = self.layer_norm_in(z)\n",
    "        if not self.training and block_size is not None:\n",
    "            return self._chunk_2d(z, mask, block_size=block_size)\n",
    "\n",
    "        g = nn.functional.linear(z, self.linear_g.weight)\n",
    "        if self.training:\n",
    "            ab = self.linear_ab_p(z) * mask * torch.sigmoid(self.linear_ab_g(z))\n",
    "        else:\n",
    "            ab = self.linear_ab_p(z)\n",
    "            ab *= mask\n",
    "            ab *= torch.sigmoid(self.linear_ab_g(z))\n",
    "        a, b = torch.chunk(ab, 2, dim=-1)\n",
    "        del z, ab\n",
    "\n",
    "        if self.outgoing:\n",
    "            a = permute_final_dims(a, (2, 0, 1))\n",
    "            b = b.transpose(-1, -3)\n",
    "        else:\n",
    "            b = permute_final_dims(b, (2, 0, 1))\n",
    "            a = a.transpose(-1, -3)\n",
    "        x = torch.matmul(a, b)\n",
    "        del a, b\n",
    "\n",
    "        x = permute_final_dims(x, (1, 2, 0))\n",
    "\n",
    "        x = self.layer_norm_out(x)\n",
    "        x = nn.functional.linear(x, self.linear_z.weight)\n",
    "        return x, g\n",
    "\n",
    "    def get_output_bias(self):\n",
    "        return self.linear_z.bias, self.linear_g.bias\n",
    "\n",
    "\n",
    "class TriangleMultiplicationOutgoing(TriangleMultiplication):\n",
    "    __init__ = partialmethod(TriangleMultiplication.__init__, outgoing=True)\n",
    "\n",
    "\n",
    "class TriangleMultiplicationIncoming(TriangleMultiplication):\n",
    "    __init__ = partialmethod(TriangleMultiplication.__init__, outgoing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwpGb2YCZ9L7"
   },
   "outputs": [],
   "source": [
    "# Modified from https://github.com/dptech-corp/Uni-Fold/blob/1a301710392ecf97991aebc3276aad9d0f77178f/unifold/modules/attentions.py#L349-L402\n",
    "\n",
    "class TriangleAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in,\n",
    "        d_hid,\n",
    "        num_heads,\n",
    "        starting,\n",
    "    ):\n",
    "        super(TriangleAttention, self).__init__()\n",
    "        self.starting = starting\n",
    "        self.layer_norm = nn.LayerNorm(d_in)\n",
    "        self.linear = nn.Linear(d_in, num_heads, bias=False)\n",
    "        self.mha = nn.MultiheadAttention(d_hid, num_heads)\n",
    "    @torch.jit.ignore\n",
    "    def _chunk(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        bias: Optional[torch.Tensor] = None,\n",
    "        chunk_size: int = None,\n",
    "    ) -> torch.Tensor:\n",
    "        return chunk_layer(\n",
    "            self.mha,\n",
    "            {\"q\": x, \"k\": x, \"v\": x, \"mask\": mask, \"bias\": bias},\n",
    "            chunk_size=chunk_size,\n",
    "            num_batch_dims=len(x.shape[:-2]),\n",
    "        )\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        attn_mask: Optional[torch.Tensor] = None,\n",
    "        chunk_size: Optional[int] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if not self.starting:\n",
    "            x = x.transpose(-2, -3)\n",
    "        x = self.layer_norm(x)\n",
    "        triangle_bias = (\n",
    "            permute_final_dims(self.linear(x), (2, 0, 1)).unsqueeze(-4).contiguous()\n",
    "        )\n",
    "        if chunk_size is not None:\n",
    "            x = self._chunk(x, attn_mask, triangle_bias, chunk_size)\n",
    "        else:\n",
    "            x = self.mha(query=x, key=x, value=x)\n",
    "\n",
    "        if not self.starting:\n",
    "            x = x.transpose(-2, -3)\n",
    "        return x\n",
    "\n",
    "    def get_output_bias(self):\n",
    "        return self.mha.get_output_bias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNV_S7NOa8O0"
   },
   "outputs": [],
   "source": [
    "class MyModelWithTriangleAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        vocab_size = 1024\n",
    "        self.vocab_size = vocab_size\n",
    "        embedding_dim = 512\n",
    "        hidden_size = 512\n",
    "        num_heads = 16\n",
    "        num_classes = 4\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.triangle_mutiplication_layer = TriangleMultiplicationOutgoing(\n",
    "            d_pair=embedding_dim,\n",
    "            d_hid=hidden_size,\n",
    "        )\n",
    "\n",
    "        self.triangle_attn_layer = TriangleAttention(\n",
    "            d_in = embedding_dim,\n",
    "            d_hid = hidden_size,\n",
    "            num_heads = num_heads,\n",
    "            starting = True\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(vocab_size+hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        #print(f\"type(x) = \", type(x))\n",
    "\n",
    "        mask = torch.ones(x.size()).to(torch.bool)  # Create an all-true mask\n",
    "\n",
    "        x = self.triangle_mutiplication_layer(x, mask)\n",
    "        x = self.triangle_attn_layer(x, mask)\n",
    "\n",
    "        #x = self.triangle_attn_layer(x)\n",
    "        #print(\"len(x) = \", len(x))\n",
    "        #assert x.shape[0] == x.shape[0], \"Batch dimension must match\"\n",
    "        #assert x.shape[1] == 1 and x.shape[1] > 1, \"Feature dimension must match\"\n",
    "\n",
    "        #print(\"x[0].shape = \", x[0].shape)\n",
    "        #print(\"x[1].shape = \", x[1].shape)\n",
    "\n",
    "        x = list(x)\n",
    "        x[0] = x[0].reshape(1, 1024, 512)\n",
    "\n",
    "        x = tuple(x)\n",
    "\n",
    "        x = torch.cat((x[0], x[1]), dim=-1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = MyModelWithTriangleAttention()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "inputs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training loop...\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    input = torch.randint(low=0, high=1, size=(1024,1)).to(torch.long)\n",
    "    inputs.append(input)\n",
    "\n",
    "    logits = model(input)\n",
    "    labels = torch.randint(low=0, high=1, size=(1,4))\n",
    "    loss = loss_fn(logits, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"epoch = {epoch} , loss = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oWmREsS1aCYp"
   },
   "outputs": [],
   "source": [
    "# Modified from code generated from AI chatbot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get sequence length\n",
    "seq_len = model.vocab_size\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot heatmap\n",
    "inputs = [input.to(torch.long).numpy() for input in inputs]\n",
    "im = ax.imshow(inputs, cmap='RdBu',\n",
    "   interpolation='nearest')\n",
    "\n",
    "# Label axes\n",
    "ax.set_xlabel('Query')\n",
    "ax.set_ylabel('Key')\n",
    "ax.set_xticks(np.arange(seq_len))\n",
    "ax.set_yticks(np.arange(seq_len))\n",
    "\n",
    "# Show triangle mask\n",
    "for i in range(seq_len):\n",
    "    for j in range(i):\n",
    "        ax.axvspan(i, seq_len, ymin=j, ymax=j+1, color='grey', alpha=0.2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywsIqmtm7zXH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

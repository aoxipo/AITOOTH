{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T09:49:56.509104Z",
     "start_time": "2023-06-07T09:49:56.495137Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T09:49:57.591925Z",
     "start_time": "2023-06-07T09:49:56.511094Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\DSIM\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from model.DQ.FLHD import *\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T09:49:57.622841Z",
     "start_time": "2023-06-07T09:49:57.597909Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Pixel_Attention(nn.Module):\n",
    "    def __init__(self,  in_channel ,middle_dim, n_linear_len = 2, mutl = 1 ,scale = 2, n_coder_blocks = 2 ,bias = False):  \n",
    "        super().__init__()\n",
    "        middle_list = []\n",
    "        middle_list.append(nn.Linear(in_channel * mutl, middle_dim, bias = bias))\n",
    "        for i in range(n_linear_len):\n",
    "            middle_list.append(nn.BatchNorm1d(scale * scale))\n",
    "            middle_list.append(nn.ReLU())\n",
    "            middle_list.append(nn.Linear(middle_dim, middle_dim, bias = bias))\n",
    "           \n",
    "        #middle_list.append( )\n",
    "        self.linear_box = nn.ModuleList()\n",
    "        linear_point = {}\n",
    "        for i in range(n_coder_blocks):\n",
    "            linear_item = nn.Sequential(*middle_list)\n",
    "            if id(linear_item) in linear_point:\n",
    "                assert False, 'memory with same local'\n",
    "\n",
    "            self.linear_box.append(linear_item)\n",
    "            linear_point[id(linear_item)] = 0\n",
    "        \n",
    "        del linear_point\n",
    "        self.out = nn.Linear(middle_dim * n_coder_blocks, in_channel * mutl, bias = bias)\n",
    "        self.outmo = nn.Conv2d(in_channel * mutl, in_channel,1,1, bias = False)\n",
    "        self.attention = Attention(mutl * in_channel, 4, 8)\n",
    "        self.scale = scale\n",
    "    def crop_tensor(self, image_pack):\n",
    "        _, _, w, h = image_pack.size()\n",
    "        a = int(w/self.scale)\n",
    "        b = int(h/self.scale)\n",
    "        t = torch.split(image_pack, a, dim = 2)\n",
    "        ans = []\n",
    "        for i in t:\n",
    "            for j in torch.split(i,b, dim=3):\n",
    "                ans.append(j)\n",
    "        d = torch.stack(ans, 1)\n",
    "        return d\n",
    "    \n",
    "    def cat_tensor(self, x,):\n",
    "        data = []\n",
    "        for i in range(self.scale):\n",
    "            m = []\n",
    "            for j in range(self.scale):\n",
    "                m.append(x[:, i * self.scale + j ,:,:,:])\n",
    "            data.append(torch.cat(m, dim = -1))\n",
    "        data = torch.cat(data, dim = -2)\n",
    "        return data\n",
    "    def forward(self, x): \n",
    "        #print('crop:', x.shape)\n",
    "        x = self.crop_tensor(x)\n",
    "        #print('croped:', x.shape)\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "        #print(x.shape)\n",
    "        attn = self.attention(x,x,x)\n",
    "        x_list = []\n",
    "        for linear_layer in self.linear_box:\n",
    "            #x_temp = linear_layer(x)\n",
    "            #print(x_temp.shape)\n",
    "            x_list.append(linear_layer(x))\n",
    "      \n",
    "        x = torch.cat([*x_list], dim = -1)\n",
    "        #print(x.shape, attn.shape)\n",
    "        x = self.out(x) * attn\n",
    "        #print(x.shape, attn.shape)\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.cat_tensor(x)\n",
    "        x = self.outmo(x)\n",
    "        return x\n",
    "class MinPool(nn.Module):\n",
    "    def __init__(self, kernel_size, ndim=2, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False):\n",
    "        super(MinPool, self).__init__()\n",
    "        self.pool = getattr(nn, f'MaxPool{ndim}d')(kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation,\n",
    "                                                  return_indices=return_indices, ceil_mode=ceil_mode)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(-x)\n",
    "        return -x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T10:45:30.367463Z",
     "start_time": "2023-06-07T10:45:30.335548Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class FLHD(PHDAE):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(FLHD, self).__init__(*args, **kwargs)\n",
    "        channel = self.channel\n",
    "        embed_dim = self.embed_dim\n",
    "        n_codebooks = self.n_codebooks\n",
    "        dec_in_channels = embed_dim * self.n_codebooks\n",
    "        n_res_block = self.n_res_block\n",
    "        n_res_channel = self.n_res_channel\n",
    "        n_coder_blocks = self.n_coder_blocks\n",
    "        stride = self.stride\n",
    "        self.focus = Focus(self.in_channel, embed_dim)\n",
    "        \n",
    "        self.high_block_list = nn.ModuleList()\n",
    "        self.high_block_list.append( Pixel_Attention( channel, 2 * channel , scale = 32, mutl=2) )\n",
    "        self.high_block_list.append( Pixel_Attention( channel, 2 * channel, scale = 16, mutl=2) )\n",
    "        self.high_block_list.append( Pixel_Attention( channel, 2 * channel, scale = 8) )\n",
    "        self.dec_blocks = nn.ModuleList()\n",
    "        for i in self.n_hier:\n",
    "            dec_blocks = [\n",
    "                Decoder(\n",
    "                    dec_in_channels,\n",
    "                    channel,\n",
    "                    channel,\n",
    "                    n_res_block,\n",
    "                    n_res_channel,\n",
    "                    stride=2,\n",
    "                )\n",
    "            ]\n",
    "            self.dec_blocks.append(nn.Sequential(*dec_blocks))\n",
    "        # ------------------------- 3, high quality image reconstruction ------------------------- #\n",
    "        upscale = 2\n",
    "\n",
    "        self.conv_after_body = nn.Conv2d(channel, embed_dim, 3, 1, 1)\n",
    "        self.conv_before_upsample = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, 12, 3, 1, 1), nn.LeakyReLU()\n",
    "        )\n",
    "        self.upsample_great = nn.Upsample(scale_factor=2)\n",
    "        self.upsample_dec = nn.Upsample(scale_factor=2)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(12, 1, 3, 1, 1)\n",
    "        \n",
    "        # ------------------------- 4, high color image ------------------------- #\n",
    "        self.attention = Attention(2048, 8, 64)\n",
    "        self.conv_great = nn.Conv2d(128,1,1,1)\n",
    "        # ------------------------- 5 erode daile ------------------------- #\n",
    "        self.erode = MinPool(2,2,1)\n",
    "        self.dilate = nn.MaxPool2d(2, stride = 1)\n",
    "        \n",
    "    def high_color(self, x):\n",
    "        print(\"scale in:\", x.shape)\n",
    "        #x = self.scale_conv(x)\n",
    "        x = self.crop_tensor(x)\n",
    "        print(\"scale out:\", x.shape)\n",
    "        #x = self.scale_conv(x_divide)\n",
    "        # print(x.shape)\n",
    "        #print(x_divide.shape, x.shape)\n",
    "        B,C1,C,W,H = x.shape\n",
    "        x1 = x.view([-1, C1 * C, W * H]).permute(0,2,1)\n",
    "        #print(x1.shape)\n",
    "        a_x = self.attention(x1,x1,x1)\n",
    "        x = a_x * x1\n",
    "        #print(x.shape)\n",
    "        x = x.view([-1, C1, C, W, H])\n",
    "        #print(x.shape)\n",
    "        x = self.cat_tensor(x)\n",
    "        #print(x.shape)\n",
    "        #x = self.descale_conv(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "        \n",
    "    def high_reconstruct(self, x):\n",
    "\n",
    "        x = self.conv_after_body(x)\n",
    "        x = self.conv_before_upsample(x)\n",
    "        x = self.conv_last(x) # 224\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        encodings = []\n",
    "        f_input = self.focus(input)\n",
    "        enc = f_input\n",
    "        for block in self.enc_blocks:\n",
    "            enc = block(enc)\n",
    "            #print(enc.shape)\n",
    "            encodings.append(enc)\n",
    "\n",
    "        # reverse them top to bot for the decoding process\n",
    "        quantize_convs = list(reversed(self.quantize_convs))\n",
    "        encodings = list(reversed(encodings))\n",
    "        dec_blocks = list(reversed(self.dec_blocks))\n",
    "        upsample_blocks = list(reversed(self.upsample))\n",
    "        high_blocks = list(reversed(self.high_block_list))\n",
    "        quants = []\n",
    "        pre_quants = []  # used for analysis of mutual information\n",
    "        ids = []\n",
    "        upsamples = []\n",
    "        # Quantizer Loss\n",
    "        diffs = 0.0\n",
    "\n",
    "        for i, enc in enumerate(encodings):\n",
    "            if i == 0:\n",
    "                # top doesn't have previous decodings to condition on\n",
    "                pass\n",
    "            else:\n",
    "                enc = torch.cat([dec, enc], 1)\n",
    "            \n",
    "            quant = high_blocks[i](enc)\n",
    "            print(\"quant:\", quant.shape)\n",
    "            dec = dec_blocks[i](quant)\n",
    "            print(\"i:\",i,\"dec :\", dec.shape)\n",
    "            upsampled = upsample_blocks[i](quant)\n",
    "            print(\"up:\", upsampled.shape)\n",
    "            upsamples.append(upsampled)\n",
    "\n",
    "        dec = self.decoder(torch.cat(upsamples, 1))\n",
    "   \n",
    "        great_color = self.high_color(dec)\n",
    "        edge = nn.functional.pad(great_color, (1, 0, 1, 0))\n",
    "        edge = self.dilate(edge) - self.erode(edge)\n",
    "        dec = self.high_reconstruct(great_color)\n",
    "        great_color = self.conv_great(edge)\n",
    "        return self.upsample_dec(dec), self.upsample_great(great_color) #, ids, (loss, recon_loss, latent_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T10:45:31.220874Z",
     "start_time": "2023-06-07T10:45:31.104157Z"
    }
   },
   "outputs": [],
   "source": [
    "model = FLHD(\n",
    "    in_channel = 1,\n",
    "    channel = 128,\n",
    "    n_res_block = 2,\n",
    "    n_res_channel = 128,\n",
    "    n_coder_blocks = 2,\n",
    "    embed_dim = 64,\n",
    "    n_codebooks = 2,\n",
    "    stride = 2,\n",
    "    decay = 0.99,\n",
    "    loss_name = \"mse\",\n",
    "    vq_type = \"dq\",\n",
    "    beta = 0.25,\n",
    "    n_hier = [64, 128, 256],\n",
    "    n_logistic_mix = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T10:45:31.775469Z",
     "start_time": "2023-06-07T10:45:31.759497Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_image = torch.zeros((2,1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T10:45:32.975092Z",
     "start_time": "2023-06-07T10:45:32.194476Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant: torch.Size([2, 128, 8, 8])\n",
      "i: 0 dec : torch.Size([2, 128, 16, 16])\n",
      "up: torch.Size([2, 128, 64, 64])\n",
      "quant: torch.Size([2, 128, 16, 16])\n",
      "i: 1 dec : torch.Size([2, 128, 32, 32])\n",
      "up: torch.Size([2, 128, 64, 64])\n",
      "quant: torch.Size([2, 128, 32, 32])\n",
      "i: 2 dec : torch.Size([2, 128, 64, 64])\n",
      "up: torch.Size([2, 128, 64, 64])\n",
      "scale in: torch.Size([2, 128, 128, 128])\n",
      "scale out: torch.Size([2, 16, 128, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "ans = model(batch_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T09:43:05.943756Z",
     "start_time": "2023-06-07T09:43:05.923801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 256, 256])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-07T09:50:05.888773Z",
     "start_time": "2023-06-07T09:50:02.460813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant: torch.Size([2, 128, 8, 8])\n",
      "i: 0 dec : torch.Size([2, 128, 16, 16])\n",
      "up: torch.Size([2, 128, 64, 64])\n",
      "quant: torch.Size([2, 128, 16, 16])\n",
      "i: 1 dec : torch.Size([2, 128, 32, 32])\n",
      "up: torch.Size([2, 128, 64, 64])\n",
      "quant: torch.Size([2, 128, 32, 32])\n",
      "i: 2 dec : torch.Size([2, 128, 64, 64])\n",
      "up: torch.Size([2, 128, 64, 64])\n",
      "scale in: torch.Size([2, 128, 128, 128])\n",
      "scale out: torch.Size([2, 16, 128, 32, 32])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 128, 128]             320\n",
      "       BatchNorm2d-2         [-1, 64, 128, 128]             128\n",
      "              SiLU-3         [-1, 64, 128, 128]               0\n",
      "             Focus-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5           [-1, 64, 64, 64]          65,600\n",
      "              ReLU-6           [-1, 64, 64, 64]               0\n",
      "            Conv2d-7          [-1, 128, 64, 64]          73,856\n",
      "              ReLU-8          [-1, 128, 64, 64]               0\n",
      "            Conv2d-9          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-10          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-11          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-12          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-13          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-14          [-1, 128, 64, 64]               0\n",
      "              CBL-15          [-1, 128, 64, 64]               0\n",
      "              CBL-16          [-1, 128, 64, 64]               0\n",
      "           Conv2d-17          [-1, 128, 64, 64]          16,512\n",
      "           Conv2d-18          [-1, 128, 64, 64]          16,512\n",
      "      BatchNorm2d-19          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-20          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-21          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-22          [-1, 128, 64, 64]               0\n",
      "              CBL-23          [-1, 128, 64, 64]               0\n",
      "              CBL-24          [-1, 128, 64, 64]               0\n",
      "           Conv2d-25          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-26          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-27          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-28          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-29          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-30          [-1, 128, 64, 64]               0\n",
      "              CBL-31          [-1, 128, 64, 64]               0\n",
      "              CBL-32          [-1, 128, 64, 64]               0\n",
      "           Conv2d-33          [-1, 128, 64, 64]          16,512\n",
      "           Conv2d-34          [-1, 128, 64, 64]          16,512\n",
      "      BatchNorm2d-35          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-36          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-37          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-38          [-1, 128, 64, 64]               0\n",
      "              CBL-39          [-1, 128, 64, 64]               0\n",
      "              CBL-40          [-1, 128, 64, 64]               0\n",
      "           Conv2d-41          [-1, 128, 64, 64]          16,512\n",
      "           Conv2d-42          [-1, 128, 64, 64]          32,896\n",
      "      BatchNorm2d-43          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-44          [-1, 128, 64, 64]               0\n",
      "     MultResBlock-45          [-1, 128, 64, 64]               0\n",
      "           Conv2d-46          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-47          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-48          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-49          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-50          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-51          [-1, 128, 64, 64]               0\n",
      "              CBL-52          [-1, 128, 64, 64]               0\n",
      "              CBL-53          [-1, 128, 64, 64]               0\n",
      "           Conv2d-54          [-1, 128, 64, 64]          16,512\n",
      "           Conv2d-55          [-1, 128, 64, 64]          16,512\n",
      "      BatchNorm2d-56          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-57          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-58          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-59          [-1, 128, 64, 64]               0\n",
      "              CBL-60          [-1, 128, 64, 64]               0\n",
      "              CBL-61          [-1, 128, 64, 64]               0\n",
      "           Conv2d-62          [-1, 128, 64, 64]         147,584\n",
      "           Conv2d-63          [-1, 128, 64, 64]         147,584\n",
      "      BatchNorm2d-64          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-65          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-66          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-67          [-1, 128, 64, 64]               0\n",
      "              CBL-68          [-1, 128, 64, 64]               0\n",
      "              CBL-69          [-1, 128, 64, 64]               0\n",
      "           Conv2d-70          [-1, 128, 64, 64]          16,512\n",
      "           Conv2d-71          [-1, 128, 64, 64]          16,512\n",
      "      BatchNorm2d-72          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-73          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-74          [-1, 128, 64, 64]               0\n",
      "        LeakyReLU-75          [-1, 128, 64, 64]               0\n",
      "              CBL-76          [-1, 128, 64, 64]               0\n",
      "              CBL-77          [-1, 128, 64, 64]               0\n",
      "           Conv2d-78          [-1, 128, 64, 64]          16,512\n",
      "           Conv2d-79          [-1, 128, 64, 64]          32,896\n",
      "      BatchNorm2d-80          [-1, 128, 64, 64]             256\n",
      "        LeakyReLU-81          [-1, 128, 64, 64]               0\n",
      "     MultResBlock-82          [-1, 128, 64, 64]               0\n",
      "             ReLU-83          [-1, 128, 64, 64]               0\n",
      "      EncoderMult-84          [-1, 128, 64, 64]               0\n",
      "           Conv2d-85           [-1, 64, 32, 32]         131,136\n",
      "             ReLU-86           [-1, 64, 32, 32]               0\n",
      "           Conv2d-87          [-1, 128, 32, 32]          73,856\n",
      "             ReLU-88          [-1, 128, 32, 32]               0\n",
      "           Conv2d-89          [-1, 128, 32, 32]         147,584\n",
      "           Conv2d-90          [-1, 128, 32, 32]         147,584\n",
      "      BatchNorm2d-91          [-1, 128, 32, 32]             256\n",
      "      BatchNorm2d-92          [-1, 128, 32, 32]             256\n",
      "        LeakyReLU-93          [-1, 128, 32, 32]               0\n",
      "        LeakyReLU-94          [-1, 128, 32, 32]               0\n",
      "              CBL-95          [-1, 128, 32, 32]               0\n",
      "              CBL-96          [-1, 128, 32, 32]               0\n",
      "           Conv2d-97          [-1, 128, 32, 32]          16,512\n",
      "           Conv2d-98          [-1, 128, 32, 32]          16,512\n",
      "      BatchNorm2d-99          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-100          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-101          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-102          [-1, 128, 32, 32]               0\n",
      "             CBL-103          [-1, 128, 32, 32]               0\n",
      "             CBL-104          [-1, 128, 32, 32]               0\n",
      "          Conv2d-105          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-106          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-107          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-108          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-109          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-110          [-1, 128, 32, 32]               0\n",
      "             CBL-111          [-1, 128, 32, 32]               0\n",
      "             CBL-112          [-1, 128, 32, 32]               0\n",
      "          Conv2d-113          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-114          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-115          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-116          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-117          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-118          [-1, 128, 32, 32]               0\n",
      "             CBL-119          [-1, 128, 32, 32]               0\n",
      "             CBL-120          [-1, 128, 32, 32]               0\n",
      "          Conv2d-121          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-122          [-1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-123          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-124          [-1, 128, 32, 32]               0\n",
      "    MultResBlock-125          [-1, 128, 32, 32]               0\n",
      "          Conv2d-126          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-127          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-128          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-129          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-130          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-131          [-1, 128, 32, 32]               0\n",
      "             CBL-132          [-1, 128, 32, 32]               0\n",
      "             CBL-133          [-1, 128, 32, 32]               0\n",
      "          Conv2d-134          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-135          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-136          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-137          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-138          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-139          [-1, 128, 32, 32]               0\n",
      "             CBL-140          [-1, 128, 32, 32]               0\n",
      "             CBL-141          [-1, 128, 32, 32]               0\n",
      "          Conv2d-142          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-143          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-144          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-145          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-146          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-147          [-1, 128, 32, 32]               0\n",
      "             CBL-148          [-1, 128, 32, 32]               0\n",
      "             CBL-149          [-1, 128, 32, 32]               0\n",
      "          Conv2d-150          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-151          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-152          [-1, 128, 32, 32]             256\n",
      "     BatchNorm2d-153          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-154          [-1, 128, 32, 32]               0\n",
      "       LeakyReLU-155          [-1, 128, 32, 32]               0\n",
      "             CBL-156          [-1, 128, 32, 32]               0\n",
      "             CBL-157          [-1, 128, 32, 32]               0\n",
      "          Conv2d-158          [-1, 128, 32, 32]          16,512\n",
      "          Conv2d-159          [-1, 128, 32, 32]          32,896\n",
      "     BatchNorm2d-160          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-161          [-1, 128, 32, 32]               0\n",
      "    MultResBlock-162          [-1, 128, 32, 32]               0\n",
      "            ReLU-163          [-1, 128, 32, 32]               0\n",
      "     EncoderMult-164          [-1, 128, 32, 32]               0\n",
      "          Conv2d-165           [-1, 64, 16, 16]         131,136\n",
      "            ReLU-166           [-1, 64, 16, 16]               0\n",
      "          Conv2d-167          [-1, 128, 16, 16]          73,856\n",
      "            ReLU-168          [-1, 128, 16, 16]               0\n",
      "          Conv2d-169          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-170          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-171          [-1, 128, 16, 16]               0\n",
      "             CBL-172          [-1, 128, 16, 16]               0\n",
      "          Conv2d-173          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-174          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-175          [-1, 128, 16, 16]               0\n",
      "             CBL-176          [-1, 128, 16, 16]               0\n",
      "            ReLU-177          [-1, 128, 16, 16]               0\n",
      "        ResBlock-178          [-1, 128, 16, 16]               0\n",
      "          Conv2d-179          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-180          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-181          [-1, 128, 16, 16]               0\n",
      "             CBL-182          [-1, 128, 16, 16]               0\n",
      "          Conv2d-183          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-184          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-185          [-1, 128, 16, 16]               0\n",
      "             CBL-186          [-1, 128, 16, 16]               0\n",
      "            ReLU-187          [-1, 128, 16, 16]               0\n",
      "        ResBlock-188          [-1, 128, 16, 16]               0\n",
      "            ReLU-189          [-1, 128, 16, 16]               0\n",
      "         Encoder-190          [-1, 128, 16, 16]               0\n",
      "          Conv2d-191             [-1, 64, 8, 8]         131,136\n",
      "            ReLU-192             [-1, 64, 8, 8]               0\n",
      "          Conv2d-193            [-1, 128, 8, 8]          73,856\n",
      "            ReLU-194            [-1, 128, 8, 8]               0\n",
      "          Conv2d-195            [-1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-196            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-197            [-1, 128, 8, 8]               0\n",
      "             CBL-198            [-1, 128, 8, 8]               0\n",
      "          Conv2d-199            [-1, 128, 8, 8]          16,512\n",
      "     BatchNorm2d-200            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-201            [-1, 128, 8, 8]               0\n",
      "             CBL-202            [-1, 128, 8, 8]               0\n",
      "            ReLU-203            [-1, 128, 8, 8]               0\n",
      "        ResBlock-204            [-1, 128, 8, 8]               0\n",
      "          Conv2d-205            [-1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-206            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-207            [-1, 128, 8, 8]               0\n",
      "             CBL-208            [-1, 128, 8, 8]               0\n",
      "          Conv2d-209            [-1, 128, 8, 8]          16,512\n",
      "     BatchNorm2d-210            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-211            [-1, 128, 8, 8]               0\n",
      "             CBL-212            [-1, 128, 8, 8]               0\n",
      "            ReLU-213            [-1, 128, 8, 8]               0\n",
      "        ResBlock-214            [-1, 128, 8, 8]               0\n",
      "            ReLU-215            [-1, 128, 8, 8]               0\n",
      "         Encoder-216            [-1, 128, 8, 8]               0\n",
      "          Linear-217               [-1, 64, 16]           2,064\n",
      "          Linear-218               [-1, 64, 16]           2,064\n",
      "          Linear-219               [-1, 64, 16]           2,064\n",
      "         Dropout-220             [-1, 4, 64, 4]               0\n",
      "          Linear-221              [-1, 64, 128]           2,176\n",
      "       Attention-222              [-1, 64, 128]               0\n",
      "          Linear-223              [-1, 64, 256]          32,768\n",
      "          Linear-224              [-1, 64, 256]          32,768\n",
      "     BatchNorm1d-225              [-1, 64, 256]             128\n",
      "     BatchNorm1d-226              [-1, 64, 256]             128\n",
      "            ReLU-227              [-1, 64, 256]               0\n",
      "            ReLU-228              [-1, 64, 256]               0\n",
      "          Linear-229              [-1, 64, 256]          65,536\n",
      "          Linear-230              [-1, 64, 256]          65,536\n",
      "     BatchNorm1d-231              [-1, 64, 256]             128\n",
      "     BatchNorm1d-232              [-1, 64, 256]             128\n",
      "            ReLU-233              [-1, 64, 256]               0\n",
      "            ReLU-234              [-1, 64, 256]               0\n",
      "          Linear-235              [-1, 64, 256]          65,536\n",
      "          Linear-236              [-1, 64, 256]          65,536\n",
      "          Linear-237              [-1, 64, 256]          32,768\n",
      "          Linear-238              [-1, 64, 256]          32,768\n",
      "     BatchNorm1d-239              [-1, 64, 256]             128\n",
      "     BatchNorm1d-240              [-1, 64, 256]             128\n",
      "            ReLU-241              [-1, 64, 256]               0\n",
      "            ReLU-242              [-1, 64, 256]               0\n",
      "          Linear-243              [-1, 64, 256]          65,536\n",
      "          Linear-244              [-1, 64, 256]          65,536\n",
      "     BatchNorm1d-245              [-1, 64, 256]             128\n",
      "     BatchNorm1d-246              [-1, 64, 256]             128\n",
      "            ReLU-247              [-1, 64, 256]               0\n",
      "            ReLU-248              [-1, 64, 256]               0\n",
      "          Linear-249              [-1, 64, 256]          65,536\n",
      "          Linear-250              [-1, 64, 256]          65,536\n",
      "          Linear-251              [-1, 64, 128]          65,536\n",
      "          Conv2d-252            [-1, 128, 8, 8]          16,384\n",
      " Pixel_Attention-253            [-1, 128, 8, 8]               0\n",
      "          Conv2d-254            [-1, 128, 8, 8]         147,584\n",
      "          Conv2d-255            [-1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-256            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-257            [-1, 128, 8, 8]               0\n",
      "             CBL-258            [-1, 128, 8, 8]               0\n",
      "          Conv2d-259            [-1, 128, 8, 8]          16,512\n",
      "     BatchNorm2d-260            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-261            [-1, 128, 8, 8]               0\n",
      "             CBL-262            [-1, 128, 8, 8]               0\n",
      "            ReLU-263            [-1, 128, 8, 8]               0\n",
      "        ResBlock-264            [-1, 128, 8, 8]               0\n",
      "          Conv2d-265            [-1, 128, 8, 8]         147,584\n",
      "     BatchNorm2d-266            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-267            [-1, 128, 8, 8]               0\n",
      "             CBL-268            [-1, 128, 8, 8]               0\n",
      "          Conv2d-269            [-1, 128, 8, 8]          16,512\n",
      "     BatchNorm2d-270            [-1, 128, 8, 8]             256\n",
      "       LeakyReLU-271            [-1, 128, 8, 8]               0\n",
      "             CBL-272            [-1, 128, 8, 8]               0\n",
      "            ReLU-273            [-1, 128, 8, 8]               0\n",
      "        ResBlock-274            [-1, 128, 8, 8]               0\n",
      "            ReLU-275            [-1, 128, 8, 8]               0\n",
      " ConvTranspose2d-276          [-1, 128, 16, 16]          65,664\n",
      "         Decoder-277          [-1, 128, 16, 16]               0\n",
      "        Upsample-278          [-1, 128, 16, 16]               0\n",
      "          Conv2d-279          [-1, 128, 16, 16]         147,584\n",
      "          Conv2d-280          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-281          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-282          [-1, 128, 16, 16]               0\n",
      "             CBL-283          [-1, 128, 16, 16]               0\n",
      "          Conv2d-284          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-285          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-286          [-1, 128, 16, 16]               0\n",
      "             CBL-287          [-1, 128, 16, 16]               0\n",
      "            ReLU-288          [-1, 128, 16, 16]               0\n",
      "        ResBlock-289          [-1, 128, 16, 16]               0\n",
      "          Conv2d-290          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-291          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-292          [-1, 128, 16, 16]               0\n",
      "             CBL-293          [-1, 128, 16, 16]               0\n",
      "          Conv2d-294          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-295          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-296          [-1, 128, 16, 16]               0\n",
      "             CBL-297          [-1, 128, 16, 16]               0\n",
      "            ReLU-298          [-1, 128, 16, 16]               0\n",
      "        ResBlock-299          [-1, 128, 16, 16]               0\n",
      "            ReLU-300          [-1, 128, 16, 16]               0\n",
      " ConvTranspose2d-301          [-1, 128, 32, 32]          65,664\n",
      "         Decoder-302          [-1, 128, 32, 32]               0\n",
      "          Conv2d-303          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-304          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-305          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-306          [-1, 128, 32, 32]               0\n",
      "             CBL-307          [-1, 128, 32, 32]               0\n",
      "          Conv2d-308          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-309          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-310          [-1, 128, 32, 32]               0\n",
      "             CBL-311          [-1, 128, 32, 32]               0\n",
      "            ReLU-312          [-1, 128, 32, 32]               0\n",
      "        ResBlock-313          [-1, 128, 32, 32]               0\n",
      "          Conv2d-314          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-315          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-316          [-1, 128, 32, 32]               0\n",
      "             CBL-317          [-1, 128, 32, 32]               0\n",
      "          Conv2d-318          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-319          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-320          [-1, 128, 32, 32]               0\n",
      "             CBL-321          [-1, 128, 32, 32]               0\n",
      "            ReLU-322          [-1, 128, 32, 32]               0\n",
      "        ResBlock-323          [-1, 128, 32, 32]               0\n",
      "            ReLU-324          [-1, 128, 32, 32]               0\n",
      " ConvTranspose2d-325          [-1, 128, 64, 64]          65,664\n",
      "         Decoder-326          [-1, 128, 64, 64]               0\n",
      "          Linear-327              [-1, 256, 32]           8,224\n",
      "          Linear-328              [-1, 256, 32]           8,224\n",
      "          Linear-329              [-1, 256, 32]           8,224\n",
      "         Dropout-330            [-1, 4, 256, 8]               0\n",
      "          Linear-331             [-1, 256, 256]           8,448\n",
      "       Attention-332             [-1, 256, 256]               0\n",
      "          Linear-333             [-1, 256, 256]          65,536\n",
      "          Linear-334             [-1, 256, 256]          65,536\n",
      "     BatchNorm1d-335             [-1, 256, 256]             512\n",
      "     BatchNorm1d-336             [-1, 256, 256]             512\n",
      "            ReLU-337             [-1, 256, 256]               0\n",
      "            ReLU-338             [-1, 256, 256]               0\n",
      "          Linear-339             [-1, 256, 256]          65,536\n",
      "          Linear-340             [-1, 256, 256]          65,536\n",
      "     BatchNorm1d-341             [-1, 256, 256]             512\n",
      "     BatchNorm1d-342             [-1, 256, 256]             512\n",
      "            ReLU-343             [-1, 256, 256]               0\n",
      "            ReLU-344             [-1, 256, 256]               0\n",
      "          Linear-345             [-1, 256, 256]          65,536\n",
      "          Linear-346             [-1, 256, 256]          65,536\n",
      "          Linear-347             [-1, 256, 256]          65,536\n",
      "          Linear-348             [-1, 256, 256]          65,536\n",
      "     BatchNorm1d-349             [-1, 256, 256]             512\n",
      "     BatchNorm1d-350             [-1, 256, 256]             512\n",
      "            ReLU-351             [-1, 256, 256]               0\n",
      "            ReLU-352             [-1, 256, 256]               0\n",
      "          Linear-353             [-1, 256, 256]          65,536\n",
      "          Linear-354             [-1, 256, 256]          65,536\n",
      "     BatchNorm1d-355             [-1, 256, 256]             512\n",
      "     BatchNorm1d-356             [-1, 256, 256]             512\n",
      "            ReLU-357             [-1, 256, 256]               0\n",
      "            ReLU-358             [-1, 256, 256]               0\n",
      "          Linear-359             [-1, 256, 256]          65,536\n",
      "          Linear-360             [-1, 256, 256]          65,536\n",
      "          Linear-361             [-1, 256, 256]         131,072\n",
      "          Conv2d-362          [-1, 128, 16, 16]          32,768\n",
      " Pixel_Attention-363          [-1, 128, 16, 16]               0\n",
      "          Conv2d-364          [-1, 128, 16, 16]         147,584\n",
      "          Conv2d-365          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-366          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-367          [-1, 128, 16, 16]               0\n",
      "             CBL-368          [-1, 128, 16, 16]               0\n",
      "          Conv2d-369          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-370          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-371          [-1, 128, 16, 16]               0\n",
      "             CBL-372          [-1, 128, 16, 16]               0\n",
      "            ReLU-373          [-1, 128, 16, 16]               0\n",
      "        ResBlock-374          [-1, 128, 16, 16]               0\n",
      "          Conv2d-375          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-376          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-377          [-1, 128, 16, 16]               0\n",
      "             CBL-378          [-1, 128, 16, 16]               0\n",
      "          Conv2d-379          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-380          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-381          [-1, 128, 16, 16]               0\n",
      "             CBL-382          [-1, 128, 16, 16]               0\n",
      "            ReLU-383          [-1, 128, 16, 16]               0\n",
      "        ResBlock-384          [-1, 128, 16, 16]               0\n",
      "            ReLU-385          [-1, 128, 16, 16]               0\n",
      " ConvTranspose2d-386          [-1, 128, 32, 32]          65,664\n",
      "         Decoder-387          [-1, 128, 32, 32]               0\n",
      "        Upsample-388          [-1, 128, 16, 16]               0\n",
      "          Conv2d-389          [-1, 128, 16, 16]         147,584\n",
      "          Conv2d-390          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-391          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-392          [-1, 128, 16, 16]               0\n",
      "             CBL-393          [-1, 128, 16, 16]               0\n",
      "          Conv2d-394          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-395          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-396          [-1, 128, 16, 16]               0\n",
      "             CBL-397          [-1, 128, 16, 16]               0\n",
      "            ReLU-398          [-1, 128, 16, 16]               0\n",
      "        ResBlock-399          [-1, 128, 16, 16]               0\n",
      "          Conv2d-400          [-1, 128, 16, 16]         147,584\n",
      "     BatchNorm2d-401          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-402          [-1, 128, 16, 16]               0\n",
      "             CBL-403          [-1, 128, 16, 16]               0\n",
      "          Conv2d-404          [-1, 128, 16, 16]          16,512\n",
      "     BatchNorm2d-405          [-1, 128, 16, 16]             256\n",
      "       LeakyReLU-406          [-1, 128, 16, 16]               0\n",
      "             CBL-407          [-1, 128, 16, 16]               0\n",
      "            ReLU-408          [-1, 128, 16, 16]               0\n",
      "        ResBlock-409          [-1, 128, 16, 16]               0\n",
      "            ReLU-410          [-1, 128, 16, 16]               0\n",
      " ConvTranspose2d-411          [-1, 128, 32, 32]          65,664\n",
      "         Decoder-412          [-1, 128, 32, 32]               0\n",
      "          Conv2d-413          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-414          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-415          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-416          [-1, 128, 32, 32]               0\n",
      "             CBL-417          [-1, 128, 32, 32]               0\n",
      "          Conv2d-418          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-419          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-420          [-1, 128, 32, 32]               0\n",
      "             CBL-421          [-1, 128, 32, 32]               0\n",
      "            ReLU-422          [-1, 128, 32, 32]               0\n",
      "        ResBlock-423          [-1, 128, 32, 32]               0\n",
      "          Conv2d-424          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-425          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-426          [-1, 128, 32, 32]               0\n",
      "             CBL-427          [-1, 128, 32, 32]               0\n",
      "          Conv2d-428          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-429          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-430          [-1, 128, 32, 32]               0\n",
      "             CBL-431          [-1, 128, 32, 32]               0\n",
      "            ReLU-432          [-1, 128, 32, 32]               0\n",
      "        ResBlock-433          [-1, 128, 32, 32]               0\n",
      "            ReLU-434          [-1, 128, 32, 32]               0\n",
      " ConvTranspose2d-435          [-1, 128, 64, 64]          65,664\n",
      "         Decoder-436          [-1, 128, 64, 64]               0\n",
      "          Linear-437             [-1, 1024, 32]           8,224\n",
      "          Linear-438             [-1, 1024, 32]           8,224\n",
      "          Linear-439             [-1, 1024, 32]           8,224\n",
      "         Dropout-440           [-1, 4, 1024, 8]               0\n",
      "          Linear-441            [-1, 1024, 256]           8,448\n",
      "       Attention-442            [-1, 1024, 256]               0\n",
      "          Linear-443            [-1, 1024, 256]          65,536\n",
      "          Linear-444            [-1, 1024, 256]          65,536\n",
      "     BatchNorm1d-445            [-1, 1024, 256]           2,048\n",
      "     BatchNorm1d-446            [-1, 1024, 256]           2,048\n",
      "            ReLU-447            [-1, 1024, 256]               0\n",
      "            ReLU-448            [-1, 1024, 256]               0\n",
      "          Linear-449            [-1, 1024, 256]          65,536\n",
      "          Linear-450            [-1, 1024, 256]          65,536\n",
      "     BatchNorm1d-451            [-1, 1024, 256]           2,048\n",
      "     BatchNorm1d-452            [-1, 1024, 256]           2,048\n",
      "            ReLU-453            [-1, 1024, 256]               0\n",
      "            ReLU-454            [-1, 1024, 256]               0\n",
      "          Linear-455            [-1, 1024, 256]          65,536\n",
      "          Linear-456            [-1, 1024, 256]          65,536\n",
      "          Linear-457            [-1, 1024, 256]          65,536\n",
      "          Linear-458            [-1, 1024, 256]          65,536\n",
      "     BatchNorm1d-459            [-1, 1024, 256]           2,048\n",
      "     BatchNorm1d-460            [-1, 1024, 256]           2,048\n",
      "            ReLU-461            [-1, 1024, 256]               0\n",
      "            ReLU-462            [-1, 1024, 256]               0\n",
      "          Linear-463            [-1, 1024, 256]          65,536\n",
      "          Linear-464            [-1, 1024, 256]          65,536\n",
      "     BatchNorm1d-465            [-1, 1024, 256]           2,048\n",
      "     BatchNorm1d-466            [-1, 1024, 256]           2,048\n",
      "            ReLU-467            [-1, 1024, 256]               0\n",
      "            ReLU-468            [-1, 1024, 256]               0\n",
      "          Linear-469            [-1, 1024, 256]          65,536\n",
      "          Linear-470            [-1, 1024, 256]          65,536\n",
      "          Linear-471            [-1, 1024, 256]         131,072\n",
      "          Conv2d-472          [-1, 128, 32, 32]          32,768\n",
      " Pixel_Attention-473          [-1, 128, 32, 32]               0\n",
      "          Conv2d-474          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-475          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-476          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-477          [-1, 128, 32, 32]               0\n",
      "             CBL-478          [-1, 128, 32, 32]               0\n",
      "          Conv2d-479          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-480          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-481          [-1, 128, 32, 32]               0\n",
      "             CBL-482          [-1, 128, 32, 32]               0\n",
      "            ReLU-483          [-1, 128, 32, 32]               0\n",
      "        ResBlock-484          [-1, 128, 32, 32]               0\n",
      "          Conv2d-485          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-486          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-487          [-1, 128, 32, 32]               0\n",
      "             CBL-488          [-1, 128, 32, 32]               0\n",
      "          Conv2d-489          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-490          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-491          [-1, 128, 32, 32]               0\n",
      "             CBL-492          [-1, 128, 32, 32]               0\n",
      "            ReLU-493          [-1, 128, 32, 32]               0\n",
      "        ResBlock-494          [-1, 128, 32, 32]               0\n",
      "            ReLU-495          [-1, 128, 32, 32]               0\n",
      " ConvTranspose2d-496          [-1, 128, 64, 64]          65,664\n",
      "         Decoder-497          [-1, 128, 64, 64]               0\n",
      "          Conv2d-498          [-1, 128, 32, 32]         147,584\n",
      "          Conv2d-499          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-500          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-501          [-1, 128, 32, 32]               0\n",
      "             CBL-502          [-1, 128, 32, 32]               0\n",
      "          Conv2d-503          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-504          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-505          [-1, 128, 32, 32]               0\n",
      "             CBL-506          [-1, 128, 32, 32]               0\n",
      "            ReLU-507          [-1, 128, 32, 32]               0\n",
      "        ResBlock-508          [-1, 128, 32, 32]               0\n",
      "          Conv2d-509          [-1, 128, 32, 32]         147,584\n",
      "     BatchNorm2d-510          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-511          [-1, 128, 32, 32]               0\n",
      "             CBL-512          [-1, 128, 32, 32]               0\n",
      "          Conv2d-513          [-1, 128, 32, 32]          16,512\n",
      "     BatchNorm2d-514          [-1, 128, 32, 32]             256\n",
      "       LeakyReLU-515          [-1, 128, 32, 32]               0\n",
      "             CBL-516          [-1, 128, 32, 32]               0\n",
      "            ReLU-517          [-1, 128, 32, 32]               0\n",
      "        ResBlock-518          [-1, 128, 32, 32]               0\n",
      "            ReLU-519          [-1, 128, 32, 32]               0\n",
      " ConvTranspose2d-520          [-1, 128, 64, 64]          65,664\n",
      "         Decoder-521          [-1, 128, 64, 64]               0\n",
      "          Conv2d-522          [-1, 128, 64, 64]         442,496\n",
      "          Conv2d-523          [-1, 128, 64, 64]         147,584\n",
      "          Conv2d-524          [-1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-525          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-526          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-527          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-528          [-1, 128, 64, 64]               0\n",
      "             CBL-529          [-1, 128, 64, 64]               0\n",
      "             CBL-530          [-1, 128, 64, 64]               0\n",
      "          Conv2d-531          [-1, 128, 64, 64]          16,512\n",
      "          Conv2d-532          [-1, 128, 64, 64]          16,512\n",
      "     BatchNorm2d-533          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-534          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-535          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-536          [-1, 128, 64, 64]               0\n",
      "             CBL-537          [-1, 128, 64, 64]               0\n",
      "             CBL-538          [-1, 128, 64, 64]               0\n",
      "          Conv2d-539          [-1, 128, 64, 64]         147,584\n",
      "          Conv2d-540          [-1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-541          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-542          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-543          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-544          [-1, 128, 64, 64]               0\n",
      "             CBL-545          [-1, 128, 64, 64]               0\n",
      "             CBL-546          [-1, 128, 64, 64]               0\n",
      "          Conv2d-547          [-1, 128, 64, 64]          16,512\n",
      "          Conv2d-548          [-1, 128, 64, 64]          16,512\n",
      "     BatchNorm2d-549          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-550          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-551          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-552          [-1, 128, 64, 64]               0\n",
      "             CBL-553          [-1, 128, 64, 64]               0\n",
      "             CBL-554          [-1, 128, 64, 64]               0\n",
      "          Conv2d-555          [-1, 128, 64, 64]          16,512\n",
      "          Conv2d-556          [-1, 128, 64, 64]          32,896\n",
      "     BatchNorm2d-557          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-558          [-1, 128, 64, 64]               0\n",
      "    MultResBlock-559          [-1, 128, 64, 64]               0\n",
      "          Conv2d-560          [-1, 128, 64, 64]         147,584\n",
      "          Conv2d-561          [-1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-562          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-563          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-564          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-565          [-1, 128, 64, 64]               0\n",
      "             CBL-566          [-1, 128, 64, 64]               0\n",
      "             CBL-567          [-1, 128, 64, 64]               0\n",
      "          Conv2d-568          [-1, 128, 64, 64]          16,512\n",
      "          Conv2d-569          [-1, 128, 64, 64]          16,512\n",
      "     BatchNorm2d-570          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-571          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-572          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-573          [-1, 128, 64, 64]               0\n",
      "             CBL-574          [-1, 128, 64, 64]               0\n",
      "             CBL-575          [-1, 128, 64, 64]               0\n",
      "          Conv2d-576          [-1, 128, 64, 64]         147,584\n",
      "          Conv2d-577          [-1, 128, 64, 64]         147,584\n",
      "     BatchNorm2d-578          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-579          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-580          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-581          [-1, 128, 64, 64]               0\n",
      "             CBL-582          [-1, 128, 64, 64]               0\n",
      "             CBL-583          [-1, 128, 64, 64]               0\n",
      "          Conv2d-584          [-1, 128, 64, 64]          16,512\n",
      "          Conv2d-585          [-1, 128, 64, 64]          16,512\n",
      "     BatchNorm2d-586          [-1, 128, 64, 64]             256\n",
      "     BatchNorm2d-587          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-588          [-1, 128, 64, 64]               0\n",
      "       LeakyReLU-589          [-1, 128, 64, 64]               0\n",
      "             CBL-590          [-1, 128, 64, 64]               0\n",
      "             CBL-591          [-1, 128, 64, 64]               0\n",
      "          Conv2d-592          [-1, 128, 64, 64]          16,512\n",
      "          Conv2d-593          [-1, 128, 64, 64]          32,896\n",
      "     BatchNorm2d-594          [-1, 128, 64, 64]             256\n",
      "       LeakyReLU-595          [-1, 128, 64, 64]               0\n",
      "    MultResBlock-596          [-1, 128, 64, 64]               0\n",
      "            ReLU-597          [-1, 128, 64, 64]               0\n",
      " ConvTranspose2d-598        [-1, 128, 128, 128]          65,664\n",
      "     DecoderMult-599        [-1, 128, 128, 128]               0\n",
      "          Linear-600             [-1, 1024, 32]          65,568\n",
      "          Linear-601             [-1, 1024, 32]          65,568\n",
      "          Linear-602             [-1, 1024, 32]          65,568\n",
      "         Dropout-603           [-1, 8, 1024, 4]               0\n",
      "          Linear-604           [-1, 1024, 2048]          67,584\n",
      "       Attention-605           [-1, 1024, 2048]               0\n",
      "       MaxPool2d-606        [-1, 128, 128, 128]               0\n",
      "       MaxPool2d-607        [-1, 128, 128, 128]               0\n",
      "         MinPool-608        [-1, 128, 128, 128]               0\n",
      "          Conv2d-609         [-1, 64, 128, 128]          73,792\n",
      "          Conv2d-610         [-1, 12, 128, 128]           6,924\n",
      "       LeakyReLU-611         [-1, 12, 128, 128]               0\n",
      "          Conv2d-612          [-1, 1, 128, 128]             109\n",
      "          Conv2d-613          [-1, 1, 128, 128]             129\n",
      "        Upsample-614          [-1, 1, 256, 256]               0\n",
      "        Upsample-615          [-1, 1, 256, 256]               0\n",
      "================================================================\n",
      "Total params: 13,588,938\n",
      "Trainable params: 13,588,938\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 1094.03\n",
      "Params size (MB): 51.84\n",
      "Estimated Total Size (MB): 1146.12\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(), (1, 256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DSIM",
   "language": "python",
   "name": "dsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

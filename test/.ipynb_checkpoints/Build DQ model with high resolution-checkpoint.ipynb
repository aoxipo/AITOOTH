{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T02:20:41.420719Z",
     "start_time": "2023-05-18T02:20:32.558060Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\DSIM\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from model.DQ.dq import *\n",
    "import math\n",
    "import datetime\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T02:20:44.043268Z",
     "start_time": "2023-05-18T02:20:44.030816Z"
    }
   },
   "outputs": [],
   "source": [
    "def crop_tensor(image_pack, scale = 4):\n",
    "    _, _, w, h = image_pack.size()\n",
    "    a = int(w/scale)\n",
    "    b = int(h/scale)\n",
    "    t = torch.split(image_pack, a, dim = 2)\n",
    "    ans = []\n",
    "    for i in t:\n",
    "        for j in torch.split(i,b, dim=3):\n",
    "            ans.append(j)\n",
    "    d = torch.stack(ans, 1)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T02:20:44.283751Z",
     "start_time": "2023-05-18T02:20:44.270777Z"
    }
   },
   "outputs": [],
   "source": [
    "def cat_tensor(x, scale = 4):\n",
    "    data = []\n",
    "    for i in range(scale):\n",
    "        m = []\n",
    "        for j in range(scale):\n",
    "            #print(i,j,i*scale + j, x[:, i*scale + j ,:,:,:].shape)\n",
    "            m.append(x[:, i*scale + j ,:,:,:])\n",
    "        data.append(torch.cat(m, dim = -1))\n",
    "    data = torch.cat(data, dim = -2)\n",
    "    #print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T02:20:45.953625Z",
     "start_time": "2023-05-18T02:20:45.934221Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def autopad(k, p=None):  # kernel, padding自动填充的设计，更加灵活多变\n",
    "    # Pad to 'same'\n",
    "    if p is None:\n",
    "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  \n",
    "# auto-pad自动填充，通过自动设置填充数p\n",
    "        #如果k是整数，p为k与2整除后向下取整；如果k是列表等，p对应的是列表中每个元素整除2。\n",
    "    return p\n",
    "class Conv(nn.Module):\n",
    "    # 这里对应结构图部分的CBL，CBL = conv+BN+Leaky ReLU，后来改成了SiLU（CBS）\n",
    "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(c2) \n",
    "#将其变为均值为0，方差为1的正态分布，通道数为c2\n",
    "        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n",
    "#其中nn.Identity()是网络中的占位符，并没有实际操作，在增减网络过程中，可以使得整个网络层数据不变，便于迁移权重数据；nn.SiLU()一种激活函数(S形加权线性单元)。\n",
    " \n",
    "    def forward(self, x):#正态分布型的前向传播\n",
    "        return self.act(self.bn(self.conv(x)))\n",
    " \n",
    "    def forward_fuse(self, x):#普通前向传播\n",
    "        return self.act(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T02:23:20.904062Z",
     "start_time": "2023-05-18T02:23:20.862842Z"
    }
   },
   "outputs": [],
   "source": [
    "a = torch.ones(1, 1, 16,16)\n",
    "a.shape\n",
    "a = a.view(-1)\n",
    "for i in range(16*16):\n",
    "    a[i]*= i + 1\n",
    "a = a.view([1,1,16,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T00:43:21.379705Z",
     "start_time": "2023-05-16T00:43:21.357302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 32, 56, 56])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(3, 32, 224,224)\n",
    "a.shape\n",
    "crop_tensor(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T00:43:21.903688Z",
     "start_time": "2023-05-16T00:43:21.884719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16, 32, 56, 56])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(3, 16, 32, 56, 56)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T00:43:25.242012Z",
     "start_time": "2023-05-16T00:43:25.230037Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that allows for downscaling the size of the embedding\n",
    "    after projection to queries, keys, and values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim: int,\n",
    "        num_heads: int,\n",
    "        downsample_rate: int = 1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.internal_dim = embedding_dim // downsample_rate\n",
    "        self.num_heads = num_heads\n",
    "        assert self.internal_dim % num_heads == 0, \"num_heads must divide embedding_dim.\"\n",
    "\n",
    "        self.q_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.k_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.v_proj = nn.Linear(embedding_dim, self.internal_dim)\n",
    "        self.out_proj = nn.Linear(self.internal_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def _separate_heads(self, x: Tensor, num_heads: int) -> Tensor:\n",
    "        b, n, c = x.shape\n",
    "        x = x.reshape(b, n, num_heads, c // num_heads)\n",
    "        return x.transpose(1, 2)  # B x N_heads x N_tokens x C_per_head\n",
    "\n",
    "    def _recombine_heads(self, x: Tensor) -> Tensor:\n",
    "        b, n_heads, n_tokens, c_per_head = x.shape\n",
    "        x = x.transpose(1, 2)\n",
    "        return x.reshape(b, n_tokens, n_heads * c_per_head)  # B x N_tokens x C\n",
    "\n",
    "    def forward(self, q: Tensor, k: Tensor, v: Tensor) -> Tensor:\n",
    "        # Input projections\n",
    "        q = self.q_proj(q)\n",
    "        k = self.k_proj(k)\n",
    "        v = self.v_proj(v)\n",
    "\n",
    "        # Separate into heads\n",
    "        q = self._separate_heads(q, self.num_heads)\n",
    "        k = self._separate_heads(k, self.num_heads)\n",
    "        v = self._separate_heads(v, self.num_heads)\n",
    "\n",
    "        # Attention\n",
    "        _, _, _, c_per_head = q.shape\n",
    "        attn = q @ k.permute(0, 1, 3, 2)  # B x N_heads x N_tokens x N_tokens\n",
    "        attn = attn / math.sqrt(c_per_head)\n",
    "        attn = torch.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # Get output\n",
    "        out = attn @ v\n",
    "        out = self._recombine_heads(out)\n",
    "        out = self.out_proj(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T00:43:25.840899Z",
     "start_time": "2023-05-16T00:43:25.820938Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Focus(nn.Module):\n",
    "    # Focus wh information into c-space\n",
    "    def __init__(self,  in_channel, out_channel, scale = 2,):  \n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d( 4 * in_channel, out_channel, 1, 1),\n",
    "            nn.BatchNorm2d(out_channel),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "        self.scale = scale\n",
    "    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n",
    "        if self.scale == 1:\n",
    "            return x\n",
    "        #print(x.shape)\n",
    "        y = torch.cat([x[..., ::self.scale, ::self.scale], x[..., 1::self.scale, ::self.scale], x[..., ::self.scale, 1::self.scale], x[..., 1::self.scale, 1::self.scale]], 1)\n",
    "        #print(y.shape)\n",
    "        y = self.conv(y)\n",
    "        #print(y.shape)\n",
    "        return y  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T01:58:18.311818Z",
     "start_time": "2023-05-17T01:58:18.301838Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\init.py:403: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    }
   ],
   "source": [
    "pa = Pixel_Attention(1, 2, scale = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T01:58:18.628566Z",
     "start_time": "2023-05-17T01:58:18.558278Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 1]) torch.Size([2, 784, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 28, 28])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.zeros(2, 1, 28, 28)\n",
    "pa(d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T02:02:14.003562Z",
     "start_time": "2023-05-17T02:02:13.983353Z"
    }
   },
   "outputs": [],
   "source": [
    "class Pixel_Attention(nn.Module):\n",
    "    def __init__(self,  in_channel, middle_dim, n_linear_len = 2, scale = 2, n_coder_blocks = 4 ,bias = False):  \n",
    "        super().__init__()\n",
    "        middle_list = []\n",
    "        middle_list.append(nn.Linear(in_channel, middle_dim, bias = bias))\n",
    "        for i in range(n_linear_len):\n",
    "            middle_list.append(nn.BatchNorm1d(scale * scale))\n",
    "            middle_list.append(nn.ReLU())\n",
    "            middle_list.append(nn.Linear(middle_dim, middle_dim, bias = bias))\n",
    "           \n",
    "        #middle_list.append( )\n",
    "        self.linear_box = []\n",
    "        linear_point = {}\n",
    "        for i in range(n_coder_blocks):\n",
    "            linear_item = nn.Sequential(*middle_list)\n",
    "            if id(linear_item) in linear_point:\n",
    "                assert False, 'memory with same local'\n",
    "\n",
    "            self.linear_box.append(linear_item)\n",
    "            linear_point[id(linear_item)] = 0\n",
    "        \n",
    "        del linear_point\n",
    "        self.out = nn.Linear(middle_dim * n_coder_blocks, in_channel, bias = bias)\n",
    "        self.attention = Attention(in_channel, 4, 4)\n",
    "        self.scale = scale\n",
    "    def crop_tensor(self, image_pack):\n",
    "        _, _, w, h = image_pack.size()\n",
    "        a = int(w/self.scale)\n",
    "        b = int(h/self.scale)\n",
    "        t = torch.split(image_pack, a, dim = 2)\n",
    "        ans = []\n",
    "        for i in t:\n",
    "            for j in torch.split(i,b, dim=3):\n",
    "                ans.append(j)\n",
    "        d = torch.stack(ans, 1)\n",
    "        return d\n",
    "    \n",
    "    def cat_tensor(self, x,):\n",
    "        data = []\n",
    "        for i in range(self.scale):\n",
    "            m = []\n",
    "            for j in range(self.scale):\n",
    "                m.append(x[:, i * self.scale + j ,:,:,:])\n",
    "            data.append(torch.cat(m, dim = -1))\n",
    "        data = torch.cat(data, dim = -2)\n",
    "        return data\n",
    "    def forward(self, x): \n",
    "        x = self.crop_tensor(x)\n",
    "        #print('croped:', x.shape)\n",
    "        x = x.squeeze(-1).squeeze(-1)\n",
    "        #print(x.shape)\n",
    "\n",
    "        attn = self.attention(x,x,x)\n",
    "        x_list = []\n",
    "        for linear_layer in self.linear_box:\n",
    "            #x_temp = linear_layer(x)\n",
    "            #print(x_temp.shape)\n",
    "            x_list.append(linear_layer(x))\n",
    "      \n",
    "        x = torch.cat([*x_list], dim = -1)\n",
    "        \n",
    "        x = self.out(x) \n",
    "        #print(x.shape, attn.shape)\n",
    "        x = x * attn\n",
    "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
    "        x = self.cat_tensor(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T01:15:25.510130Z",
     "start_time": "2023-05-17T01:15:25.502130Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:05:58.042498Z",
     "start_time": "2023-05-18T09:05:58.008004Z"
    },
    "code_folding": [
     83,
     95,
     105,
     125
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class HDAE(VQVAE):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(HDAE, self).__init__(*args, **kwargs)\n",
    "        channel = self.channel\n",
    "        embed_dim = self.embed_dim\n",
    "        n_codebooks = self.n_codebooks\n",
    "        dec_in_channels = embed_dim * self.n_codebooks\n",
    "        n_res_block = self.n_res_block\n",
    "        n_res_channel = self.n_res_channel\n",
    "        n_coder_blocks = self.n_coder_blocks\n",
    "        stride = self.stride\n",
    "        \n",
    "        self.focus = Focus(embed_dim, embed_dim)\n",
    "        self.conv_before_feature = nn.Sequential(\n",
    "            nn.Conv2d( self.in_channel, embed_dim, 1, 1),\n",
    "        )\n",
    "        \n",
    "        del self.quantize_convs\n",
    "        del self.enc_blocks\n",
    "        del self.decoder\n",
    "        \n",
    "        self.enc_blocks = nn.ModuleList()\n",
    "        self.quantize_convs = torch.nn.ModuleList()\n",
    "\n",
    "        enc_blocks = [\n",
    "            Encoder(embed_dim, channel, n_res_block, n_res_channel, stride = stride)\n",
    "        ]\n",
    "        enc_blocks += [\n",
    "            Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
    "            for i in range(n_coder_blocks - 1)\n",
    "        ]\n",
    "        self.enc_blocks.append(nn.Sequential(*enc_blocks))\n",
    "        \n",
    "        self.high_block_list = nn.ModuleList()\n",
    "        self.high_block_list.append( Pixel_Attention( channel, channel * 2, scale = 28) )\n",
    "        self.high_block_list.append( Pixel_Attention( channel, 2 * channel, scale = 14) )\n",
    "        \n",
    "        self.conv_down_1x1 = nn.ModuleList()\n",
    "        self.conv_down_1x1.append( nn.Identity() )\n",
    "        self.conv_down_1x1.append( nn.Conv2d(2*channel, channel, 1, 1) )\n",
    "        #self.conv_down_1x1.append( high_color( channel, 2 * channel, scale = 14) )\n",
    "        \n",
    "        # bot to top, excluding top because top doesn't accept a concat input\n",
    "        for i, _ in enumerate(self.n_hier):\n",
    "            if i != 0:\n",
    "                enc_blocks = [\n",
    "                    Encoder(channel, channel, n_res_block, n_res_channel, stride=2)\n",
    "                ]\n",
    "                self.enc_blocks.append(nn.Sequential(*enc_blocks))\n",
    "\n",
    "            cur_hier = len(self.n_hier) - 1 - i\n",
    "            # only for top we have channel as input to quantizer because we don't condition on prior codes\n",
    "            conv2D_channels = channel if cur_hier == 0 else channel * 2\n",
    "            quantize_conv = torch.nn.ModuleList(\n",
    "                [nn.Conv2d(conv2D_channels, embed_dim, 1) for _ in range(n_codebooks)]\n",
    "            )\n",
    "\n",
    "            self.quantize_convs.append(quantize_conv)\n",
    "        \n",
    "        dec_blocks = [\n",
    "            Decoder(\n",
    "                channel * len(self.n_hier), channel, channel, n_res_block, n_res_channel, stride = stride\n",
    "            ) for i in range(n_coder_blocks - 1)\n",
    "        ]\n",
    "\n",
    "        self.decoder = nn.Sequential(*dec_blocks)\n",
    "\n",
    "        # ------------------------- 3, high quality image reconstruction ------------------------- #\n",
    "        upscale = 2\n",
    "        num_feat = 64\n",
    "        self.conv_after_body = nn.Conv2d(channel, embed_dim, 3, 1, 1)\n",
    "        \n",
    "        self.conv_before_upsample = nn.Sequential(\n",
    "                nn.Conv2d(embed_dim, 12, 3, 1, 1), nn.LeakyReLU()\n",
    "                )\n",
    "        self.upsample_with_high = Upsample(upscale, 12)\n",
    "        self.conv_last = nn.Conv2d(12, 3, 3, 1, 1)\n",
    "        \n",
    "        # ------------------------- 4, high color image ------------------------- #\n",
    "        self.scale_conv = nn.Conv3d(16, 16, 2,2)\n",
    "        self.attention = Attention(1024, 8, 16)\n",
    "        self.descale_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_feat, channel, 4, 2, 1),\n",
    "            nn.ConvTranspose2d(channel, channel, 4, 2, 1),\n",
    "        )\n",
    "        \n",
    "        self.conv_great = nn.Conv2d(128,3,1,1)\n",
    "        \n",
    "    def crop_tensor(self, image_pack, scale = 4):\n",
    "        _, _, w, h = image_pack.size()\n",
    "        a = int(w/scale)\n",
    "        b = int(h/scale)\n",
    "        t = torch.split(image_pack, a, dim = 2)\n",
    "        ans = []\n",
    "        for i in t:\n",
    "            for j in torch.split(i,b, dim=3):\n",
    "                ans.append(j)\n",
    "        d = torch.stack(ans, 1)\n",
    "        return d\n",
    "    \n",
    "    def cat_tensor(self, x, scale = 4):\n",
    "        data = []\n",
    "        for i in range(scale):\n",
    "            m = []\n",
    "            for j in range(scale):\n",
    "                m.append(x[:, i*scale + j ,:,:,:])\n",
    "            data.append(torch.cat(m, dim = -1))\n",
    "        data = torch.cat(data, dim = -2)\n",
    "        return data\n",
    "        \n",
    "    def high_color(self, x):\n",
    "        print(\"scale in:\", x.shape)\n",
    "        x_divide = self.crop_tensor(x)\n",
    "        print(\"scale out:\", x_divide.shape)\n",
    "        x = self.scale_conv(x_divide)\n",
    "       \n",
    "        #print(x_divide.shape, x.shape)\n",
    "        B,C1,C,W,H = x.shape\n",
    "        x1 = x.view([-1, C1 * C, W * H]).permute(0,2,1)\n",
    "        #print(x1.shape)\n",
    "        a_x = self.attention(x1,x1,x1)\n",
    "        x = a_x * x1\n",
    "        #print(x.shape)\n",
    "        x = x.view([-1, C1, C, W, H])\n",
    "        #print(x.shape)\n",
    "        x = self.cat_tensor(x)\n",
    "        x = self.descale_conv(x)\n",
    "        #print(x.shape)\n",
    "        return x\n",
    "        \n",
    "    def high_reconstruct(self, x, origin_image):\n",
    "        # x 256 224 224 origin 256,224,224\n",
    "        print(origin_image.shape, x.shape)\n",
    "        x = origin_image + self.conv_after_body(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_before_upsample(x)\n",
    "        print(x.shape)\n",
    "        x = self.upsample_with_high(x)\n",
    "        print(x.shape)\n",
    "        x = self.conv_last(x)\n",
    "        print(x.shape)\n",
    "        # 3 448 448\n",
    "        return x\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        encodings = []\n",
    "        origin_image = self.conv_before_feature(input)\n",
    "        f_input = self.focus(origin_image)\n",
    "        enc = f_input\n",
    "        #print()\n",
    "        # bot to top encoder blocks and encodings\n",
    "        for block in self.enc_blocks:\n",
    "            enc = block(enc)\n",
    "            print(enc.shape)\n",
    "            encodings.append(enc)\n",
    "\n",
    "        # reverse them top to bot for the decoding process\n",
    "        quantize_convs = list(reversed(self.quantize_convs))\n",
    "        quantizers = list(reversed(self.quantizers))\n",
    "        encodings = list(reversed(encodings))\n",
    "        dec_blocks = list(reversed(self.dec_blocks))\n",
    "        upsample_blocks = list(reversed(self.upsample))\n",
    "        high_blocks = list(reversed(self.high_block_list))\n",
    "        quants = []\n",
    "        pre_quants = []  # used for analysis of mutual information\n",
    "        ids = []\n",
    "        upsamples = []\n",
    "        # Quantizer Loss\n",
    "        diffs = 0.0\n",
    "\n",
    "        for i, enc in enumerate(encodings):\n",
    "            if i == 0:\n",
    "                # top doesn't have previous decodings to condition on\n",
    "                pass\n",
    "            else:\n",
    "                enc = torch.cat([dec, enc], 1)\n",
    "                #print('before:', enc.shape)\n",
    "                #enc = self.conv_down_1x1[i](enc)\n",
    "                #print('after:', enc.shape)\n",
    "            \n",
    "            quant = self.quantize(\n",
    "                quantize_convs[i], quantizers[i], enc\n",
    "            )\n",
    "            #print(f'dec {i}:', enc.shape)\n",
    "            #quant = high_blocks[i](enc)\n",
    "            #print('quant:' , quant.shape)\n",
    "            dec = dec_blocks[i](quant)\n",
    "            upsampled = upsample_blocks[i](quant)\n",
    "            upsamples.append(upsampled)\n",
    "\n",
    "        dec = self.decoder(upsampled) if self.type else self.decoder(torch.cat(upsamples, 1))\n",
    "        dec = self.dropout(dec)\n",
    "     \n",
    "        great_color = self.high_color(dec)\n",
    "        \n",
    "        dec = self.high_reconstruct(great_color, origin_image)\n",
    "        great_color = self.conv_great(great_color)\n",
    "        return dec, great_color, diffs #, ids, (loss, recon_loss, latent_loss)\n",
    "\n",
    "    def quantize(self, conv_block, quant_block, input):\n",
    "        quants = []\n",
    "        #diff = 0.0\n",
    "        #ids = []\n",
    "        pre_quants = []\n",
    "        #print('input:{}'.format(input.shape))\n",
    "        for i in range(self.n_codebooks):\n",
    "            pre_quant = conv_block[i](input)\n",
    "            #.permute(0, 2, 3, 1)\n",
    "            #print('pre_quant:{}'.format(pre_quant.shape))\n",
    "            #quant_i, diff_i, idx = quant_block[i](pre_quant)\n",
    "            quant_i = pre_quant# quant_i.permute(0, 3, 1, 2)\n",
    "            #diff_i = diff_i.unsqueeze(0)\n",
    "            #diff += diff_i\n",
    "            quants.append(quant_i)\n",
    "            #ids.append(idx)\n",
    "            pre_quants.append(pre_quant.permute(0, 3, 1, 2))\n",
    "\n",
    "        #ids = torch.stack(ids, 1)\n",
    "        quants = torch.cat(quants, 1)\n",
    "        #print('quants:{}'.format(quants.shape))\n",
    "        return quants #diff, ids, torch.cat(pre_quants, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:05:58.752891Z",
     "start_time": "2023-05-18T09:05:58.641064Z"
    }
   },
   "outputs": [],
   "source": [
    "model = HDAE(\n",
    "in_channel = 3,\n",
    "channel = 128,\n",
    "n_res_block = 3,\n",
    "n_res_channel = 128,\n",
    "n_coder_blocks = 2,\n",
    "embed_dim = 32,\n",
    "n_codebooks = 4,\n",
    "stride = 2,\n",
    "decay = 0.99,\n",
    "loss_name = \"mse\",\n",
    "vq_type = \"dq\",\n",
    "beta = 0.25,\n",
    "n_hier = [64, 128],\n",
    "n_logistic_mix = 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:05:59.277170Z",
     "start_time": "2023-05-18T09:05:59.265089Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_image = torch.randn((1,3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T09:06:00.494945Z",
     "start_time": "2023-05-18T09:05:59.855594Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 28, 28])\n",
      "torch.Size([1, 128, 14, 14])\n",
      "scale in: torch.Size([1, 128, 112, 112])\n",
      "scale out: torch.Size([1, 16, 128, 28, 28])\n",
      "torch.Size([1, 32, 224, 224]) torch.Size([1, 128, 224, 224])\n",
      "torch.Size([1, 32, 224, 224])\n",
      "torch.Size([1, 12, 224, 224])\n",
      "torch.Size([1, 12, 448, 448])\n",
      "torch.Size([1, 3, 448, 448])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 448, 448]), torch.Size([1, 3, 224, 224]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = model(batch_image)\n",
    "ans[0].shape, ans[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T00:43:42.654761Z",
     "start_time": "2023-05-16T00:43:42.221904Z"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T00:44:02.990351Z",
     "start_time": "2023-05-16T00:43:42.765459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 28, 28])\n",
      "torch.Size([2, 128, 14, 14])\n",
      "dec 0: torch.Size([2, 128, 14, 14])\n",
      "quant: torch.Size([2, 128, 14, 14])\n",
      "dec 1: torch.Size([2, 128, 28, 28])\n",
      "quant: torch.Size([2, 128, 28, 28])\n",
      "scale in: torch.Size([2, 128, 112, 112])\n",
      "scale out: torch.Size([2, 16, 128, 28, 28])\n",
      "torch.Size([2, 32, 224, 224]) torch.Size([2, 128, 224, 224])\n",
      "torch.Size([2, 32, 224, 224])\n",
      "torch.Size([2, 12, 224, 224])\n",
      "torch.Size([2, 12, 448, 448])\n",
      "torch.Size([2, 3, 448, 448])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             128\n",
      "            Conv2d-2         [-1, 32, 112, 112]           4,128\n",
      "       BatchNorm2d-3         [-1, 32, 112, 112]              64\n",
      "              SiLU-4         [-1, 32, 112, 112]               0\n",
      "             Focus-5         [-1, 32, 112, 112]               0\n",
      "            Conv2d-6           [-1, 64, 56, 56]          32,832\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8          [-1, 128, 56, 56]          73,856\n",
      "              ReLU-9          [-1, 128, 56, 56]               0\n",
      "           Conv2d-10          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-12          [-1, 128, 56, 56]               0\n",
      "              CBL-13          [-1, 128, 56, 56]               0\n",
      "           Conv2d-14          [-1, 128, 56, 56]          16,512\n",
      "      BatchNorm2d-15          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-16          [-1, 128, 56, 56]               0\n",
      "              CBL-17          [-1, 128, 56, 56]               0\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "         ResBlock-19          [-1, 128, 56, 56]               0\n",
      "           Conv2d-20          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-21          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-22          [-1, 128, 56, 56]               0\n",
      "              CBL-23          [-1, 128, 56, 56]               0\n",
      "           Conv2d-24          [-1, 128, 56, 56]          16,512\n",
      "      BatchNorm2d-25          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-26          [-1, 128, 56, 56]               0\n",
      "              CBL-27          [-1, 128, 56, 56]               0\n",
      "             ReLU-28          [-1, 128, 56, 56]               0\n",
      "         ResBlock-29          [-1, 128, 56, 56]               0\n",
      "           Conv2d-30          [-1, 128, 56, 56]         147,584\n",
      "      BatchNorm2d-31          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-32          [-1, 128, 56, 56]               0\n",
      "              CBL-33          [-1, 128, 56, 56]               0\n",
      "           Conv2d-34          [-1, 128, 56, 56]          16,512\n",
      "      BatchNorm2d-35          [-1, 128, 56, 56]             256\n",
      "        LeakyReLU-36          [-1, 128, 56, 56]               0\n",
      "              CBL-37          [-1, 128, 56, 56]               0\n",
      "             ReLU-38          [-1, 128, 56, 56]               0\n",
      "         ResBlock-39          [-1, 128, 56, 56]               0\n",
      "             ReLU-40          [-1, 128, 56, 56]               0\n",
      "          Encoder-41          [-1, 128, 56, 56]               0\n",
      "           Conv2d-42           [-1, 64, 28, 28]         131,136\n",
      "             ReLU-43           [-1, 64, 28, 28]               0\n",
      "           Conv2d-44          [-1, 128, 28, 28]          73,856\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-47          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-48          [-1, 128, 28, 28]               0\n",
      "              CBL-49          [-1, 128, 28, 28]               0\n",
      "           Conv2d-50          [-1, 128, 28, 28]          16,512\n",
      "      BatchNorm2d-51          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-52          [-1, 128, 28, 28]               0\n",
      "              CBL-53          [-1, 128, 28, 28]               0\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "         ResBlock-55          [-1, 128, 28, 28]               0\n",
      "           Conv2d-56          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-57          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-58          [-1, 128, 28, 28]               0\n",
      "              CBL-59          [-1, 128, 28, 28]               0\n",
      "           Conv2d-60          [-1, 128, 28, 28]          16,512\n",
      "      BatchNorm2d-61          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-62          [-1, 128, 28, 28]               0\n",
      "              CBL-63          [-1, 128, 28, 28]               0\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "         ResBlock-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,584\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-68          [-1, 128, 28, 28]               0\n",
      "              CBL-69          [-1, 128, 28, 28]               0\n",
      "           Conv2d-70          [-1, 128, 28, 28]          16,512\n",
      "      BatchNorm2d-71          [-1, 128, 28, 28]             256\n",
      "        LeakyReLU-72          [-1, 128, 28, 28]               0\n",
      "              CBL-73          [-1, 128, 28, 28]               0\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "         ResBlock-75          [-1, 128, 28, 28]               0\n",
      "             ReLU-76          [-1, 128, 28, 28]               0\n",
      "          Encoder-77          [-1, 128, 28, 28]               0\n",
      "           Conv2d-78           [-1, 64, 14, 14]         131,136\n",
      "             ReLU-79           [-1, 64, 14, 14]               0\n",
      "           Conv2d-80          [-1, 128, 14, 14]          73,856\n",
      "             ReLU-81          [-1, 128, 14, 14]               0\n",
      "           Conv2d-82          [-1, 128, 14, 14]         147,584\n",
      "      BatchNorm2d-83          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-84          [-1, 128, 14, 14]               0\n",
      "              CBL-85          [-1, 128, 14, 14]               0\n",
      "           Conv2d-86          [-1, 128, 14, 14]          16,512\n",
      "      BatchNorm2d-87          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-88          [-1, 128, 14, 14]               0\n",
      "              CBL-89          [-1, 128, 14, 14]               0\n",
      "             ReLU-90          [-1, 128, 14, 14]               0\n",
      "         ResBlock-91          [-1, 128, 14, 14]               0\n",
      "           Conv2d-92          [-1, 128, 14, 14]         147,584\n",
      "      BatchNorm2d-93          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-94          [-1, 128, 14, 14]               0\n",
      "              CBL-95          [-1, 128, 14, 14]               0\n",
      "           Conv2d-96          [-1, 128, 14, 14]          16,512\n",
      "      BatchNorm2d-97          [-1, 128, 14, 14]             256\n",
      "        LeakyReLU-98          [-1, 128, 14, 14]               0\n",
      "              CBL-99          [-1, 128, 14, 14]               0\n",
      "            ReLU-100          [-1, 128, 14, 14]               0\n",
      "        ResBlock-101          [-1, 128, 14, 14]               0\n",
      "          Conv2d-102          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-103          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-104          [-1, 128, 14, 14]               0\n",
      "             CBL-105          [-1, 128, 14, 14]               0\n",
      "          Conv2d-106          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-107          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-108          [-1, 128, 14, 14]               0\n",
      "             CBL-109          [-1, 128, 14, 14]               0\n",
      "            ReLU-110          [-1, 128, 14, 14]               0\n",
      "        ResBlock-111          [-1, 128, 14, 14]               0\n",
      "            ReLU-112          [-1, 128, 14, 14]               0\n",
      "         Encoder-113          [-1, 128, 14, 14]               0\n",
      "          Linear-114             [-1, 196, 256]          32,768\n",
      "     BatchNorm1d-115             [-1, 196, 256]             392\n",
      "            ReLU-116             [-1, 196, 256]               0\n",
      "          Linear-117             [-1, 196, 256]          65,536\n",
      "     BatchNorm1d-118             [-1, 196, 256]             392\n",
      "            ReLU-119             [-1, 196, 256]               0\n",
      "          Linear-120             [-1, 196, 256]          65,536\n",
      "          Linear-121             [-1, 196, 128]          32,768\n",
      "      high_color-122          [-1, 128, 14, 14]               0\n",
      "          Conv2d-123          [-1, 128, 14, 14]         147,584\n",
      "          Conv2d-124          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-125          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-126          [-1, 128, 14, 14]               0\n",
      "             CBL-127          [-1, 128, 14, 14]               0\n",
      "          Conv2d-128          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-129          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-130          [-1, 128, 14, 14]               0\n",
      "             CBL-131          [-1, 128, 14, 14]               0\n",
      "            ReLU-132          [-1, 128, 14, 14]               0\n",
      "        ResBlock-133          [-1, 128, 14, 14]               0\n",
      "          Conv2d-134          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-135          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-136          [-1, 128, 14, 14]               0\n",
      "             CBL-137          [-1, 128, 14, 14]               0\n",
      "          Conv2d-138          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-139          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-140          [-1, 128, 14, 14]               0\n",
      "             CBL-141          [-1, 128, 14, 14]               0\n",
      "            ReLU-142          [-1, 128, 14, 14]               0\n",
      "        ResBlock-143          [-1, 128, 14, 14]               0\n",
      "          Conv2d-144          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-145          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-146          [-1, 128, 14, 14]               0\n",
      "             CBL-147          [-1, 128, 14, 14]               0\n",
      "          Conv2d-148          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-149          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-150          [-1, 128, 14, 14]               0\n",
      "             CBL-151          [-1, 128, 14, 14]               0\n",
      "            ReLU-152          [-1, 128, 14, 14]               0\n",
      "        ResBlock-153          [-1, 128, 14, 14]               0\n",
      "            ReLU-154          [-1, 128, 14, 14]               0\n",
      " ConvTranspose2d-155          [-1, 128, 28, 28]          65,664\n",
      "         Decoder-156          [-1, 128, 28, 28]               0\n",
      "        Upsample-157          [-1, 128, 14, 14]               0\n",
      "          Conv2d-158          [-1, 128, 14, 14]         147,584\n",
      "          Conv2d-159          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-160          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-161          [-1, 128, 14, 14]               0\n",
      "             CBL-162          [-1, 128, 14, 14]               0\n",
      "          Conv2d-163          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-164          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-165          [-1, 128, 14, 14]               0\n",
      "             CBL-166          [-1, 128, 14, 14]               0\n",
      "            ReLU-167          [-1, 128, 14, 14]               0\n",
      "        ResBlock-168          [-1, 128, 14, 14]               0\n",
      "          Conv2d-169          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-170          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-171          [-1, 128, 14, 14]               0\n",
      "             CBL-172          [-1, 128, 14, 14]               0\n",
      "          Conv2d-173          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-174          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-175          [-1, 128, 14, 14]               0\n",
      "             CBL-176          [-1, 128, 14, 14]               0\n",
      "            ReLU-177          [-1, 128, 14, 14]               0\n",
      "        ResBlock-178          [-1, 128, 14, 14]               0\n",
      "          Conv2d-179          [-1, 128, 14, 14]         147,584\n",
      "     BatchNorm2d-180          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-181          [-1, 128, 14, 14]               0\n",
      "             CBL-182          [-1, 128, 14, 14]               0\n",
      "          Conv2d-183          [-1, 128, 14, 14]          16,512\n",
      "     BatchNorm2d-184          [-1, 128, 14, 14]             256\n",
      "       LeakyReLU-185          [-1, 128, 14, 14]               0\n",
      "             CBL-186          [-1, 128, 14, 14]               0\n",
      "            ReLU-187          [-1, 128, 14, 14]               0\n",
      "        ResBlock-188          [-1, 128, 14, 14]               0\n",
      "            ReLU-189          [-1, 128, 14, 14]               0\n",
      " ConvTranspose2d-190          [-1, 128, 28, 28]          65,664\n",
      "         Decoder-191          [-1, 128, 28, 28]               0\n",
      "          Conv2d-192          [-1, 128, 28, 28]         147,584\n",
      "          Conv2d-193          [-1, 128, 28, 28]         147,584\n",
      "     BatchNorm2d-194          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-195          [-1, 128, 28, 28]               0\n",
      "             CBL-196          [-1, 128, 28, 28]               0\n",
      "          Conv2d-197          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-198          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-199          [-1, 128, 28, 28]               0\n",
      "             CBL-200          [-1, 128, 28, 28]               0\n",
      "            ReLU-201          [-1, 128, 28, 28]               0\n",
      "        ResBlock-202          [-1, 128, 28, 28]               0\n",
      "          Conv2d-203          [-1, 128, 28, 28]         147,584\n",
      "     BatchNorm2d-204          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-205          [-1, 128, 28, 28]               0\n",
      "             CBL-206          [-1, 128, 28, 28]               0\n",
      "          Conv2d-207          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-208          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-209          [-1, 128, 28, 28]               0\n",
      "             CBL-210          [-1, 128, 28, 28]               0\n",
      "            ReLU-211          [-1, 128, 28, 28]               0\n",
      "        ResBlock-212          [-1, 128, 28, 28]               0\n",
      "          Conv2d-213          [-1, 128, 28, 28]         147,584\n",
      "     BatchNorm2d-214          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-215          [-1, 128, 28, 28]               0\n",
      "             CBL-216          [-1, 128, 28, 28]               0\n",
      "          Conv2d-217          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-218          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-219          [-1, 128, 28, 28]               0\n",
      "             CBL-220          [-1, 128, 28, 28]               0\n",
      "            ReLU-221          [-1, 128, 28, 28]               0\n",
      "        ResBlock-222          [-1, 128, 28, 28]               0\n",
      "            ReLU-223          [-1, 128, 28, 28]               0\n",
      " ConvTranspose2d-224          [-1, 128, 56, 56]          65,664\n",
      "         Decoder-225          [-1, 128, 56, 56]               0\n",
      "          Conv2d-226          [-1, 128, 28, 28]          32,896\n",
      "          Linear-227             [-1, 784, 256]          32,768\n",
      "     BatchNorm1d-228             [-1, 784, 256]           1,568\n",
      "            ReLU-229             [-1, 784, 256]               0\n",
      "          Linear-230             [-1, 784, 256]          65,536\n",
      "     BatchNorm1d-231             [-1, 784, 256]           1,568\n",
      "            ReLU-232             [-1, 784, 256]               0\n",
      "          Linear-233             [-1, 784, 256]          65,536\n",
      "          Linear-234             [-1, 784, 128]          32,768\n",
      "      high_color-235          [-1, 128, 28, 28]               0\n",
      "        Identity-236          [-1, 128, 28, 28]               0\n",
      "          Conv2d-237          [-1, 128, 28, 28]         147,584\n",
      "          Conv2d-238          [-1, 128, 28, 28]         147,584\n",
      "     BatchNorm2d-239          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-240          [-1, 128, 28, 28]               0\n",
      "             CBL-241          [-1, 128, 28, 28]               0\n",
      "          Conv2d-242          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-243          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-244          [-1, 128, 28, 28]               0\n",
      "             CBL-245          [-1, 128, 28, 28]               0\n",
      "            ReLU-246          [-1, 128, 28, 28]               0\n",
      "        ResBlock-247          [-1, 128, 28, 28]               0\n",
      "          Conv2d-248          [-1, 128, 28, 28]         147,584\n",
      "     BatchNorm2d-249          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-250          [-1, 128, 28, 28]               0\n",
      "             CBL-251          [-1, 128, 28, 28]               0\n",
      "          Conv2d-252          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-253          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-254          [-1, 128, 28, 28]               0\n",
      "             CBL-255          [-1, 128, 28, 28]               0\n",
      "            ReLU-256          [-1, 128, 28, 28]               0\n",
      "        ResBlock-257          [-1, 128, 28, 28]               0\n",
      "          Conv2d-258          [-1, 128, 28, 28]         147,584\n",
      "     BatchNorm2d-259          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-260          [-1, 128, 28, 28]               0\n",
      "             CBL-261          [-1, 128, 28, 28]               0\n",
      "          Conv2d-262          [-1, 128, 28, 28]          16,512\n",
      "     BatchNorm2d-263          [-1, 128, 28, 28]             256\n",
      "       LeakyReLU-264          [-1, 128, 28, 28]               0\n",
      "             CBL-265          [-1, 128, 28, 28]               0\n",
      "            ReLU-266          [-1, 128, 28, 28]               0\n",
      "        ResBlock-267          [-1, 128, 28, 28]               0\n",
      "            ReLU-268          [-1, 128, 28, 28]               0\n",
      " ConvTranspose2d-269          [-1, 128, 56, 56]          65,664\n",
      "         Decoder-270          [-1, 128, 56, 56]               0\n",
      "          Conv2d-271          [-1, 128, 56, 56]         295,040\n",
      "          Conv2d-272          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-273          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-274          [-1, 128, 56, 56]               0\n",
      "             CBL-275          [-1, 128, 56, 56]               0\n",
      "          Conv2d-276          [-1, 128, 56, 56]          16,512\n",
      "     BatchNorm2d-277          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-278          [-1, 128, 56, 56]               0\n",
      "             CBL-279          [-1, 128, 56, 56]               0\n",
      "            ReLU-280          [-1, 128, 56, 56]               0\n",
      "        ResBlock-281          [-1, 128, 56, 56]               0\n",
      "          Conv2d-282          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-283          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-284          [-1, 128, 56, 56]               0\n",
      "             CBL-285          [-1, 128, 56, 56]               0\n",
      "          Conv2d-286          [-1, 128, 56, 56]          16,512\n",
      "     BatchNorm2d-287          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-288          [-1, 128, 56, 56]               0\n",
      "             CBL-289          [-1, 128, 56, 56]               0\n",
      "            ReLU-290          [-1, 128, 56, 56]               0\n",
      "        ResBlock-291          [-1, 128, 56, 56]               0\n",
      "          Conv2d-292          [-1, 128, 56, 56]         147,584\n",
      "     BatchNorm2d-293          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-294          [-1, 128, 56, 56]               0\n",
      "             CBL-295          [-1, 128, 56, 56]               0\n",
      "          Conv2d-296          [-1, 128, 56, 56]          16,512\n",
      "     BatchNorm2d-297          [-1, 128, 56, 56]             256\n",
      "       LeakyReLU-298          [-1, 128, 56, 56]               0\n",
      "             CBL-299          [-1, 128, 56, 56]               0\n",
      "            ReLU-300          [-1, 128, 56, 56]               0\n",
      "        ResBlock-301          [-1, 128, 56, 56]               0\n",
      "            ReLU-302          [-1, 128, 56, 56]               0\n",
      " ConvTranspose2d-303        [-1, 128, 112, 112]          65,664\n",
      "         Decoder-304        [-1, 128, 112, 112]               0\n",
      "       Dropout2d-305        [-1, 128, 112, 112]               0\n",
      "          Conv3d-306       [-1, 16, 64, 14, 14]           2,064\n",
      "          Linear-307              [-1, 196, 64]          65,600\n",
      "          Linear-308              [-1, 196, 64]          65,600\n",
      "          Linear-309              [-1, 196, 64]          65,600\n",
      "         Dropout-310          [-1, 8, 196, 196]               0\n",
      "          Linear-311            [-1, 196, 1024]          66,560\n",
      "       Attention-312            [-1, 196, 1024]               0\n",
      " ConvTranspose2d-313        [-1, 128, 112, 112]         131,200\n",
      " ConvTranspose2d-314        [-1, 128, 224, 224]         262,272\n",
      "          Conv2d-315         [-1, 32, 224, 224]          36,896\n",
      "          Conv2d-316         [-1, 12, 224, 224]           3,468\n",
      "       LeakyReLU-317         [-1, 12, 224, 224]               0\n",
      "          Conv2d-318         [-1, 48, 224, 224]           5,232\n",
      "    PixelShuffle-319         [-1, 12, 448, 448]               0\n",
      "          Conv2d-320          [-1, 3, 448, 448]             327\n",
      "          Conv2d-321          [-1, 3, 224, 224]             387\n",
      "================================================================\n",
      "Total params: 6,820,518\n",
      "Trainable params: 6,820,518\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 525.94\n",
      "Params size (MB): 26.02\n",
      "Estimated Total Size (MB): 552.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:11:19.319225Z",
     "start_time": "2023-05-15T08:11:19.306257Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:02:44.121848Z",
     "start_time": "2023-05-15T08:02:44.104388Z"
    }
   },
   "outputs": [],
   "source": [
    "d = torch.zeros(2, 128 ,28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:02:44.451735Z",
     "start_time": "2023-05-15T08:02:44.440264Z"
    }
   },
   "outputs": [],
   "source": [
    "c = high_color(128, 128 * 2, scale = 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:02:47.825429Z",
     "start_time": "2023-05-15T08:02:47.783889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 784, 128, 1, 1])\n",
      "torch.Size([2, 784, 128])\n",
      "torch.Size([2, 784, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 28, 28])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T03:25:42.221522Z",
     "start_time": "2023-05-15T03:25:42.210392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 128])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = d.view([1,  28*28, 128])\n",
    "d1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T03:29:54.319855Z",
     "start_time": "2023-05-15T03:29:54.308896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 784, 128])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.crop_tensor(d,28).squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T03:29:57.460317Z",
     "start_time": "2023-05-15T03:29:57.444189Z"
    }
   },
   "outputs": [],
   "source": [
    "c = nn.Linear(128, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T03:29:58.046859Z",
     "start_time": "2023-05-15T03:29:58.034425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784, 256])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(d1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T02:47:29.446741Z",
     "start_time": "2023-05-15T02:47:29.442752Z"
    }
   },
   "outputs": [],
   "source": [
    "c = nn.Conv2d(1,1,(32,32),(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T02:47:29.688804Z",
     "start_time": "2023-05-15T02:47:29.664868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 128, 128])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-19T08:41:50.739381Z",
     "start_time": "2023-04-19T08:41:50.733409Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(obj = c.state_dict(), f='test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T01:12:38.957150Z",
     "start_time": "2023-04-26T01:12:38.595087Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(obj = model.state_dict(), f='test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T12:53:42.611288Z",
     "start_time": "2023-03-27T12:53:42.493590Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 256 elements not 128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m112\u001b[39m,\u001b[38;5;241m112\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mH:\\program\\outpage\\AIColor\\test\\..\\model\\DQ\\dq.py:204\u001b[0m, in \u001b[0;36mResBlock.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m--> 204\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     out \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\DSIM\\lib\\site-packages\\torch\\nn\\functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2419\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2422\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2423\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: running_mean should contain 256 elements not 128"
     ]
    }
   ],
   "source": [
    "d = torch.zeros((1,256,112,112))\n",
    "model.decoder[2](d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T12:51:15.707952Z",
     "start_time": "2023-03-27T12:51:14.590898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 448, 448])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bat = torch.zeros((1,256,224,224))\n",
    "model.decoder[1](bat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:07:51.927178Z",
     "start_time": "2023-03-27T09:07:51.908041Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:30:12.579000Z",
     "start_time": "2023-03-27T09:30:11.798035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 112, 112])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros((1,256,224,224))\n",
    "model.decoder[1](c).shape\n",
    "nn.Conv2d(256, 1, 2, 2)(c).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T03:42:07.675408Z",
     "start_time": "2023-04-26T03:42:07.371216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 48, 12544])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = torch.zeros(1, 3*16, 16*28*28).to('cuda:0')\n",
    "model = Attention(12544, 8, 16).to('cuda:0')\n",
    "model(im,im,im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-26T03:40:00.778468Z",
     "start_time": "2023-04-26T03:39:56.297638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 48, 784]       9,835,280\n",
      "            Linear-2              [-1, 48, 784]       9,835,280\n",
      "            Linear-3              [-1, 48, 784]       9,835,280\n",
      "            Linear-4            [-1, 48, 12544]       9,847,040\n",
      "================================================================\n",
      "Total params: 39,352,880\n",
      "Trainable params: 39,352,880\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 5.46\n",
      "Params size (MB): 150.12\n",
      "Estimated Total Size (MB): 155.57\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Attention(12544, 8, 16).to('cuda:0')\n",
    "summary(model, [[3*16, 16*28*28], [3*16, 16*28*28], [3*16, 16*28*28]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:40:04.643825Z",
     "start_time": "2023-03-27T09:40:04.630862Z"
    }
   },
   "outputs": [],
   "source": [
    "o = torch.zeros((1,1,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:40:27.419205Z",
     "start_time": "2023-03-27T09:40:27.399109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2016/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T01:19:16.961187Z",
     "start_time": "2023-05-17T01:19:16.942122Z"
    }
   },
   "outputs": [],
   "source": [
    "d = nn.Conv2d(1,1,1,1)\n",
    "d_list = [d]\n",
    "c = nn.Sequential(d)\n",
    "c1 = nn.Sequential(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T01:19:32.513878Z",
     "start_time": "2023-05-17T01:19:32.494697Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543643296768"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T01:19:38.446297Z",
     "start_time": "2023-05-17T01:19:38.431325Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2543643371664"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:37:00.729079Z",
     "start_time": "2023-03-27T09:37:00.710554Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.dec_blocks),len(model.decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:37:23.957701Z",
     "start_time": "2023-03-27T09:37:23.943750Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 64, 128, 256]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.n_hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-03-27T09:32:19.492145Z",
     "start_time": "2023-03-27T09:32:19.462044Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 336, 336])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "channel = 1\n",
    "c = nn.Sequential(\n",
    "    nn.ConvTranspose2d(channel, channel, 3, stride=3),\n",
    "    nn.Conv2d(channel, channel, 3, 3),\n",
    "    nn.ConvTranspose2d(channel, channel, 3, stride=3),\n",
    "    nn.Conv2d(channel, channel, 2, 2),\n",
    "    nn.ConvTranspose2d(channel, channel, 2, stride=2),\n",
    "    nn.Conv2d(channel, channel, 2, 2),\n",
    ")\n",
    "a = torch.zeros((1,1,224,224))\n",
    "c(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "DSIM",
   "language": "python",
   "name": "dsim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
